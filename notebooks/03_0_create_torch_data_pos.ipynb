{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a313373-38fd-4b77-93b3-1a3d3ffba023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import skimage\n",
    "from skimage import io\n",
    "from sklearn import preprocessing\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import anndata as ad\n",
    "import cv2\n",
    "import scanorama\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ca6630-d585-470e-8bbd-91831f8c07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spatial omics library\n",
    "import athena as ath\n",
    "from spatialOmics import SpatialOmics\n",
    "\n",
    "# import default graph builder parameters\n",
    "from athena.graph_builder.constants import GRAPH_BUILDER_DEFAULT_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ff4633-5c72-4133-ab1c-127ff917ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x000001CE63CB6460>\n",
      "2\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check pytorch\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976c11-3c7b-4787-bc87-3e4f5e0f7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "\n",
    "spatial_omics_folder = (Path().cwd().parents[0]).absolute() / 'data' / 'spatial_omics_graph'\n",
    "process_path = (Path().cwd().parents[0]).absolute() / 'data' / 'torch_graph_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662db41-a41e-4357-8994-bcfac0213cc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4075ab79-d658-4f35-b452-4caa8bca302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12fac3aea0e4e89b3b7b410c2a295c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read all spadata info\n",
    "datasets = []\n",
    "paths = []\n",
    "path_rois = []\n",
    "\n",
    "# Loop through data\n",
    "for (dirpath, dirnames, filenames) in os.walk(spatial_omics_folder):\n",
    "    for name in tqdm(sorted(filenames)):\n",
    "        if 'hdf5' not in name:\n",
    "            continue\n",
    "         \n",
    "        # read adata\n",
    "        data_name = name.split('.')[0]\n",
    "        path = os.path.join(dirpath, name)\n",
    "        if 'roi' not in name:\n",
    "            datasets.append(data_name)\n",
    "            paths.append(os.path.join(dirpath, name))\n",
    "            path_rois.append(os.path.join(dirpath, data_name+'_roi.hdf5'))\n",
    "            \n",
    "df = pd.DataFrame({'Dataset': datasets, 'Path': paths, 'Path_ROI': path_rois})\n",
    "df = df[df.Dataset!= '05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565484d7-7038-40e2-9557-0bc6c381f9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7deea058ed6449f7934a9a6f95552405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data 01\n",
      "Processing Data 03\n",
      "Processing Data 05_v1\n",
      "Processing Data 07_A11\n",
      "Processing Data 07_A18\n",
      "Processing Data 07_A21\n",
      "Processing Data 07_A22\n",
      "Processing Data 07_A6\n",
      "Processing Data 07_A8\n",
      "Processing Data 07_T18\n",
      "Processing Data 07_T22\n",
      "Processing Data 07_T3\n",
      "Processing Data 07_T5\n",
      "Processing Data 07_T6\n",
      "Processing Data 07_T8\n",
      "Processing Data 08\n"
     ]
    }
   ],
   "source": [
    "adata_dir = (Path().cwd().parents[0]).absolute() / 'data'/ 'processed'\n",
    "adatas = []\n",
    "datasets = []\n",
    "\n",
    "# Loop through data\n",
    "for (dirpath, dirnames, filenames) in os.walk(adata_dir):\n",
    "    for name in tqdm(sorted(filenames)):\n",
    "        if 'h5ad' not in name:\n",
    "            continue\n",
    "        if name == '05.h5ad':\n",
    "            continue\n",
    "        \n",
    "        # read adata\n",
    "        data_name = name.split('.')[0]\n",
    "        print(f'Processing Data {data_name}')   \n",
    "        path = os.path.join(dirpath, name)\n",
    "        \n",
    "        adata = sc.read_h5ad(path)\n",
    "        adata.obs['Data'] = data_name\n",
    "        \n",
    "        if 'Dataset' in adata.obs:\n",
    "            for d in adata.obs.Dataset.unique():\n",
    "                a = adata[adata.obs.Dataset == d]\n",
    "                adatas.append(a)\n",
    "                datasets.append(data_name+f'_{d}')\n",
    "        else:\n",
    "            adatas.append(adata)\n",
    "            datasets.append(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fcde2-d3e3-49a5-8e62-966ce9194c3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6001f42-8276-4cc7-976d-fb8070805a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\anndata\\_core\\anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "keep_names = ['cd11c', 'cd20', 'cd21', 'hladr', 'cd68', 'ki67', 'cd19']\n",
    "final_names = ['CD21', 'CD20', 'CD21', 'CD68', 'CD68', 'Ki67', 'CD20']\n",
    "mapping = dict(zip(keep_names, final_names))\n",
    "\n",
    "adatas_filtered = []\n",
    "for adata in adatas:\n",
    "    lower_var_names = adata.var_names.map(lambda x: x.split('_')[0]).str.lower()\n",
    "    indices = lower_var_names.isin(keep_names)\n",
    "    adata_s = adata[:, indices]\n",
    "    if len(adata_s.var_names) > 4:\n",
    "        indices = lower_var_names.isin(keep_names[:-1])\n",
    "        adata_s = adata[:, indices]\n",
    "    columns = adata_s.var_names.map(lambda x: x.split('_')[0]).str.lower()\n",
    "    columns = columns.map(mapping)\n",
    "    adata_s.var_names = columns\n",
    "    sc.pp.scale(adata_s, max_value=5)\n",
    "    adatas_filtered.append(adata_s)\n",
    "    \n",
    "adata_combined = ad.concat(adatas_filtered, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbaacf18-c55a-4681-a661-91c5388c0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'Data' as categorical\n"
     ]
    }
   ],
   "source": [
    "sc.pp.combat(adata_combined, key='Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8612b71-600a-4d95-a896-1329b62eae65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create torch dataset Follicle only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b6e3ba-24cc-4c39-8bc4-0f1d84305409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01', '03', '05_v1', '07_A11', '07_A18', ..., '07_T3', '07_T5', '07_T6', '07_T8', '08']\n",
       "Length: 16\n",
       "Categories (16, object): ['01', '03', '05_v1', '07_A6', ..., '07_T8', '07_T18', '07_T22', '08']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined_foll = adata_combined[adata_combined.obs.Foll > 0]\n",
    "adata_combined_foll.obs.Data.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea91b2-d1fa-4e90-8786-2a446550c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv for all graph info\n",
    "save_path = process_path / 'All' \n",
    "pt_path = save_path / 'pt'\n",
    "pt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define tissue types \n",
    "# 0: Tonsil, 1: Adenoid\n",
    "tissue_types = [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "assert len(tissue_types) == len(df)\n",
    "\n",
    "# Create csv for all graph info\n",
    "csv_path = save_path / 'info.csv'\n",
    "\n",
    "test_size = 0.35\n",
    "\n",
    "data_names = []\n",
    "foll_ids = []\n",
    "paths = []\n",
    "spadata_paths = []\n",
    "spadata_roi_paths = []\n",
    "\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "    dataset = row.Dataset\n",
    "    \n",
    "    # Read spadata\n",
    "    path = row.Path\n",
    "    spadata = SpatialOmics.from_h5py(path)\n",
    "    adata_dataset = adata_combined_foll[adata_combined_foll.obs.Data == dataset]\n",
    "    \n",
    "    # Loop through foll\n",
    "    for foll in spadata.spl.Foll:\n",
    "        # Filter out foll with less than 50 cells\n",
    "        l = len(spadata.X[str(foll)])\n",
    "        if l < 50:\n",
    "            continue \n",
    "            \n",
    "        # Generate torch data\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        pos = spadata.obs[str(foll)][['x','y']].loc[np.array(G.nodes())].to_numpy()\n",
    "        classes = (spadata.obs[str(foll)]['GC']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        adata_subset = adata_dataset[adata_dataset.obs.Foll == foll]\n",
    "        df_int = pd.DataFrame(adata_subset.X, index=adata_subset.obs.Cell)\n",
    "        X = df_int.loc[np.array(G.nodes)].values\n",
    "        \n",
    "        data = torch_geometric.utils.from_networkx(spadata.G[str(foll)]['contact'])\n",
    "        data.x = torch.tensor(X).float()\n",
    "        data.pos = torch.tensor(pos).float()\n",
    "        data.node_types = torch.tensor(classes)\n",
    "        data.label = torch.tensor(tissue_types[i])\n",
    "        \n",
    "        # Generate train and test mask at node level\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        X_train, X_test, _, _ = train_test_split(pd.Series(list(G.nodes())), \n",
    "                                                pd.Series(spadata.obs[str(foll)]['GC']),\n",
    "                                                test_size=test_size, \n",
    "                                                random_state=42)\n",
    "\n",
    "        n_nodes = G.number_of_nodes()\n",
    "\n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train.index] = True\n",
    "        test_mask[X_test.index] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "        \n",
    "        # Save data and info\n",
    "        path = save_path / 'pt' / f'{dataset}_{foll}.pt'\n",
    "        torch.save(data, path)\n",
    "        data_names.append(dataset)\n",
    "        foll_ids.append(foll)\n",
    "        paths.append(path)\n",
    "        spadata_paths.append(row.Path)\n",
    "        spadata_roi_paths.append(row.Path_ROI)\n",
    "    \n",
    "df_info = pd.DataFrame({'Dataset': data_names, 'Foll': foll_ids, 'Path': paths,\n",
    "                       'Path_spadata': spadata_paths, 'Path_spadata_roi':spadata_roi_paths})\n",
    "df_info.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80374b84-b2d2-4a03-a56e-9117ae585e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info['fileName'] = df_info['Path'].map(lambda x: str(x).split('\\\\')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c5111-1556-46ea-a913-0aec3021fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7196834-b061-4870-9f93-de0d11d67eac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create torch dataset Follicle ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39434b7-9aea-4df5-a999-7235b548407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv for all graph info\n",
    "save_path = process_path / 'All_roi' \n",
    "pt_path = save_path / 'pt'\n",
    "pt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define tissue types \n",
    "# 0: Tonsil, 1: Adenoid\n",
    "tissue_types = [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "assert len(tissue_types) == len(df)\n",
    "\n",
    "# Create csv for all graph info\n",
    "csv_path = save_path / 'info.csv'\n",
    "\n",
    "test_size = 0.35\n",
    "\n",
    "data_names = []\n",
    "foll_ids = []\n",
    "paths = []\n",
    "spadata_paths = []\n",
    "spadata_roi_paths = []\n",
    "\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "    dataset = row.Dataset\n",
    "    \n",
    "    # Read spadata\n",
    "    path = row.Path_ROI\n",
    "    spadata = SpatialOmics.from_h5py(path)\n",
    "    adata_dataset = adata_combined[adata_combined.obs.Data == dataset]\n",
    "    df_int = pd.DataFrame(adata_dataset.X, index=adata_dataset.obs.Cell)\n",
    "\n",
    "    # Loop through foll\n",
    "    for foll in spadata.spl.Foll:\n",
    "        # Filter out foll with less than 50 cells\n",
    "        l = len(spadata.X[str(foll)])\n",
    "        if l < 50:\n",
    "            continue \n",
    "    \n",
    "        # Generate torch data\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        pos = spadata.obs[str(foll)][['x','y']].loc[np.array(G.nodes())].to_numpy()\n",
    "        gc_onehot = (spadata.obs[str(foll)]['GC']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        foll_onehot = (spadata.obs[str(foll)]['Foll']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        classes = gc_onehot + foll_onehot\n",
    "        X = df_int.loc[np.array(G.nodes)].values\n",
    "        \n",
    "        data = torch_geometric.utils.from_networkx(spadata.G[str(foll)]['contact'])\n",
    "        data.x = torch.tensor(X).float()\n",
    "        data.pos = torch.tensor(pos).float()\n",
    "        data.node_types = torch.tensor(classes)\n",
    "        data.label = torch.tensor(tissue_types[i])\n",
    "        \n",
    "        # Generate train and test mask at node level\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        X_train, X_test, _, _ = train_test_split(pd.Series(list(G.nodes())), \n",
    "                                                pd.Series(spadata.obs[str(foll)]['GC']),\n",
    "                                                test_size=test_size, \n",
    "                                                random_state=42)\n",
    "\n",
    "        n_nodes = G.number_of_nodes()\n",
    "\n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train.index] = True\n",
    "        test_mask[X_test.index] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "        \n",
    "        # Save data and info\n",
    "        path = pt_path / f'{dataset}_{foll}.pt'\n",
    "        torch.save(data, path)\n",
    "        data_names.append(dataset)\n",
    "        foll_ids.append(foll)\n",
    "        paths.append(path)\n",
    "        spadata_paths.append(row.Path)\n",
    "        spadata_roi_paths.append(row.Path_ROI)\n",
    "    \n",
    "df_info = pd.DataFrame({'Dataset': data_names, 'Foll': foll_ids, 'Path': paths,\n",
    "                       'Path_spadata': spadata_paths, 'Path_spadata_roi':spadata_roi_paths})\n",
    "df_info.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713239f0-01df-4adb-9393-7dda8ae76a8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NIH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91fa554d-ac2e-487b-87ad-129e7487d706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07_A11_A11\n",
      "07_A18_A18\n",
      "07_A21_A21\n",
      "07_A22_A22\n",
      "07_A6_A6\n",
      "07_A8_A8\n",
      "07_T18_T18\n",
      "07_T22_T22\n",
      "07_T3_T3\n",
      "07_T5_T5\n",
      "07_T6_T6\n",
      "07_T8_T8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\anndata\\_core\\anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "# Get MIH data\n",
    "nih_adatas = []\n",
    "for i, adata in enumerate(adatas):\n",
    "    if '07' in datasets[i]:\n",
    "        print(datasets[i])\n",
    "        nih_adatas.append(adata)\n",
    "    \n",
    "# Scale adata and combine\n",
    "adatas_filtered = []\n",
    "for adata in nih_adatas:\n",
    "    sc.pp.scale(adata, max_value=5)\n",
    "    adatas_filtered.append(adata)\n",
    "    \n",
    "adata_combined = ad.concat(nih_adatas, axis=0)    \n",
    "adata_combined = adata_combined[:,['CD21', 'CD20', 'CD68', 'Ki67', 'CD138']]\n",
    "# sc.pp.combat(adata_combined, key='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a63c41-1913-4943-b711-6d54a8c92663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nih = df[df.Dataset.str.contains('07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448d4713-e351-46b8-bfe3-bae620fa7bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Path</th>\n",
       "      <th>Path_ROI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07_A11</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07_A18</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07_A21</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>07_A22</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>07_A6</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>07_A8</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>07_T18</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>07_T22</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>07_T3</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>07_T5</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>07_T6</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>07_T8</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "      <td>Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset                                               Path  \\\n",
       "4   07_A11  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "5   07_A18  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "6   07_A21  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "7   07_A22  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "8    07_A6  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "9    07_A8  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "10  07_T18  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "11  07_T22  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "12   07_T3  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "13   07_T5  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "14   07_T6  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "15   07_T8  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...   \n",
       "\n",
       "                                             Path_ROI  \n",
       "4   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "5   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "6   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "7   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "8   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "9   Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "10  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "11  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "12  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "13  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "14  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  \n",
       "15  Y:\\coskun-lab\\Thomas\\11_snowflakes\\data\\spatia...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7866c4-fa9d-423a-80fa-b902a01edd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a00394-91b9-4c83-b86e-89551862c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 4728342 × 5\n",
       "    obs: 'ROI', 'Cell', 'Cell_ROI', 'Dataset', 'Data', 'Foll', 'GC'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a54111c-c3e9-4ae9-a84a-e2d4a8b318a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_subset = adata_combined[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d879b76d-09ba-460f-9b32-1913d93a80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata_subset)\n",
    "# sc.tl.umap(adata_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6549e6a7-57ae-4ef6-8e0b-2c419ed64abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot umap to quickly verify that we don't have clear seperation of datasets\n",
    "# sc.pl.umap(adata_subset, color=['Dataset'], size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae29d544-8ce6-495a-a158-9a313966521c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CD21', 'CD20', 'CD68', 'Ki67', 'CD138'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e7b1f-c814-41a4-bc4a-2452a11e9d0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Foll region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97114d1a-d644-489d-9d73-f47ea9582685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import LocalCartesian, Cartesian, Polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f518495a-98be-4538-b1eb-1c3268e594c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create csv for all graph info\n",
    "save_path = process_path / 'NIH_pos' \n",
    "pt_path = save_path / 'pt'\n",
    "pt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define tissue types \n",
    "# 0: Tonsil, 1: Adenoid\n",
    "tissue_types = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "covid = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "assert len(tissue_types) == len(df_nih)\n",
    "assert len(covid) == len(df_nih)\n",
    "\n",
    "# Create csv for all graph info\n",
    "csv_path = save_path / 'info.csv'\n",
    "\n",
    "test_size = 0.35\n",
    "\n",
    "data_names = []\n",
    "foll_ids = []\n",
    "paths = []\n",
    "spadata_paths = []\n",
    "spadata_roi_paths = []\n",
    "\n",
    "for i, row in enumerate(df_nih.itertuples()):\n",
    "    dataset = row.Dataset\n",
    "    \n",
    "    # Read spadata\n",
    "    path = row.Path\n",
    "    spadata = SpatialOmics.from_h5py(path)\n",
    "    adata_dataset = adata_combined[adata_combined.obs.Data == dataset]\n",
    "    \n",
    "    # Loop through foll\n",
    "    for foll in spadata.spl.Foll:\n",
    "        # Filter out foll with less than 50 cells\n",
    "        l = len(spadata.X[str(foll)])\n",
    "        if l < 50:\n",
    "            continue \n",
    "            \n",
    "        # Generate torch data\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        pos = spadata.obs[str(foll)][['x','y']].loc[np.array(G.nodes())].to_numpy()\n",
    "        classes = (spadata.obs[str(foll)]['GC']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        adata_subset = adata_dataset[adata_dataset.obs.Foll == foll]\n",
    "        df_int = pd.DataFrame(adata_subset.X, index=adata_subset.obs.Cell)\n",
    "        X = df_int.loc[np.array(G.nodes)].values\n",
    "        \n",
    "        data = torch_geometric.utils.from_networkx(spadata.G[str(foll)]['contact'])\n",
    "        data.x = torch.tensor(X).float()\n",
    "        data.pos = torch.tensor(pos).float()\n",
    "        data.node_types = torch.tensor(classes)\n",
    "        data.label = torch.tensor(tissue_types[i])\n",
    "        data.covid = torch.tensor(covid[i])\n",
    "        \n",
    "        # Generate train and test mask at node level\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        X_train, X_test, _, _ = train_test_split(pd.Series(list(G.nodes())), \n",
    "                                                pd.Series(spadata.obs[str(foll)]['GC']),\n",
    "                                                test_size=test_size, \n",
    "                                                random_state=42)\n",
    "\n",
    "        n_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train.index] = True\n",
    "        test_mask[X_test.index] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "        \n",
    "        # Polar transform to be added to the data\n",
    "        pos_transform = Polar()\n",
    "        data  = pos_transform(data)\n",
    "        \n",
    "        # Save data and info\n",
    "        path = pt_path / f'{dataset}_{foll}.pt'\n",
    "        torch.save(data, path)\n",
    "        data_names.append(dataset)\n",
    "        foll_ids.append(foll)\n",
    "        paths.append(path)\n",
    "        spadata_paths.append(row.Path)\n",
    "        spadata_roi_paths.append(row.Path_ROI)\n",
    "    \n",
    "df_info = pd.DataFrame({'Dataset': data_names, 'Foll': foll_ids, 'Path': paths,\n",
    "                       'Path_spadata': spadata_paths, 'Path_spadata_roi':spadata_roi_paths})\n",
    "df_info.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cde5db-5549-4fda-9b48-213654cfdc12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Foll ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bc0ea1-2da4-4d38-a6d9-8945b64d477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv for all graph info\n",
    "save_path = process_path / 'NIH_roi' \n",
    "pt_path = save_path / 'pt'\n",
    "pt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define tissue types \n",
    "# 0: Tonsil, 1: Adenoid\n",
    "tissue_types = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "covid = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "assert len(tissue_types) == len(df_nih)\n",
    "assert len(covid) == len(df_nih)\n",
    "\n",
    "# Create csv for all graph info\n",
    "csv_path = save_path / 'info.csv'\n",
    "\n",
    "test_size = 0.35\n",
    "\n",
    "data_names = []\n",
    "foll_ids = []\n",
    "paths = []\n",
    "spadata_paths = []\n",
    "spadata_roi_paths = []\n",
    "\n",
    "for i, row in enumerate(df_nih.itertuples()):\n",
    "    dataset = row.Dataset\n",
    "    \n",
    "    # Read spadata\n",
    "    path = row.Path_ROI\n",
    "    spadata = SpatialOmics.from_h5py(path)\n",
    "    adata_dataset = adata_combined[adata_combined.obs.Data == dataset]\n",
    "    df_int = pd.DataFrame(adata_dataset.X, index=adata_dataset.obs.Cell)\n",
    "\n",
    "    # Loop through foll\n",
    "    for foll in spadata.spl.Foll:\n",
    "        # Filter out foll with less than 50 cells\n",
    "        l = len(spadata.X[str(foll)])\n",
    "        if l < 50:\n",
    "            continue \n",
    "    \n",
    "        # Generate torch data\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        pos = spadata.obs[str(foll)][['x','y']].loc[np.array(G.nodes())].to_numpy()\n",
    "        gc_onehot = (spadata.obs[str(foll)]['GC']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        foll_onehot = (spadata.obs[str(foll)]['Foll']>0).loc[np.array(G.nodes())].astype(np.uint8).to_numpy()\n",
    "        classes = gc_onehot + foll_onehot\n",
    "        X = df_int.loc[np.array(G.nodes)].values\n",
    "        \n",
    "        data = torch_geometric.utils.from_networkx(spadata.G[str(foll)]['contact'])\n",
    "        data.x = torch.tensor(X).float()\n",
    "        data.pos = torch.tensor(pos).float()\n",
    "        data.node_types = torch.tensor(classes)\n",
    "        data.label = torch.tensor(tissue_types[i])\n",
    "        data.covid = torch.tensor(covid[i])\n",
    "        \n",
    "        # Generate train and test mask at node level\n",
    "        G = spadata.G[str(foll)]['contact']\n",
    "        X_train, X_test, _, _ = train_test_split(pd.Series(list(G.nodes())), \n",
    "                                                pd.Series(spadata.obs[str(foll)]['GC']),\n",
    "                                                test_size=test_size, \n",
    "                                                random_state=42)\n",
    "\n",
    "        n_nodes = G.number_of_nodes()\n",
    "\n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train.index] = True\n",
    "        test_mask[X_test.index] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "        \n",
    "        # Save data and info\n",
    "        path = pt_path / f'{dataset}_{foll}.pt'\n",
    "        torch.save(data, path)\n",
    "        data_names.append(dataset)\n",
    "        foll_ids.append(foll)\n",
    "        paths.append(path)\n",
    "        spadata_paths.append(row.Path)\n",
    "        spadata_roi_paths.append(row.Path_ROI)\n",
    "    \n",
    "df_info = pd.DataFrame({'Dataset': data_names, 'Foll': foll_ids, 'Path': paths,\n",
    "                       'Path_spadata': spadata_paths, 'Path_spadata_roi':spadata_roi_paths})\n",
    "df_info.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0d08e-cdcd-4dcc-afe4-bfd828c8d175",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9be84-7dfe-4a58-9f57-c03f17646bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "torch_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
