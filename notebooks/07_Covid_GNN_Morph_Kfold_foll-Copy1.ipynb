{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3aecbee-c071-4421-afbd-c038be0ee2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0483ac9b-cfc2-450a-a7b3-893fbeb4f469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7ae200-66e4-425a-bb9c-0bc533e7497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0247c074-b220-4b09-a90c-1d53c9f00a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import skimage\n",
    "from skimage import io\n",
    "from sklearn import preprocessing\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import anndata as ad\n",
    "import cv2\n",
    "import scanorama\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74a433b-63c7-46b3-a938-8b4584bb2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spatial omics library\n",
    "import athena as ath\n",
    "from spatialOmics import SpatialOmics\n",
    "\n",
    "# import default graph builder parameters\n",
    "from athena.graph_builder.constants import GRAPH_BUILDER_DEFAULT_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b279b1a4-1739-42a1-8d9e-a7c49b976e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dir = (Path().cwd().parents[0].parents[0]).absolute()\n",
    "data_dir = d_dir / \"09_datasets\"\n",
    "\n",
    "p_dir = (Path().cwd().parents[0]).absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae1ea5a-e6da-48fe-a609-173c5fcbb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84319f5d-8717-45f8-bc44-6a1321905222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "import networkx as nx\n",
    "import lightning.pytorch as pl\n",
    "import torch.utils.data as data\n",
    "\n",
    "spatial_omics_folder = (Path().cwd().parents[0]).absolute() / 'data' / 'spatial_omics_graph'\n",
    "process_path = (Path().cwd().parents[0]).absolute() / 'data' / 'torch_graph_data'\n",
    "morph_path = (Path().cwd().parents[0]).absolute() / 'data' / 'morph' / 'outlinePCA.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c3e67-8407-46cf-9f27-5420d401e8ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7f9fbf-af33-4f0f-94b0-ce345fc8c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "name = 'NIH'\n",
    "\n",
    "# Crate dataset\n",
    "dataset = graph.GraphDatasetMorph(process_path / name, morph_path, process_path / name / 'info.csv', 2, y_name='covid')\n",
    "\n",
    "train_set, val_set, test_set = graph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d076fd9b-5d82-46dd-999e-4a3a6ac32fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDatasetMorph(442):\n",
      "======================\n",
      "Number of graphs: 442\n",
      "Number of features: 5\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c215697d-363f-45f6-b561-800d2a8db3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 213, val set: 176, val set: 53\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, val set: {len(test_set)}, val set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "439ea1cb-f87e-4bb8-83e1-453647eae5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 705777], num_nodes=101577, x=[101577, 5], pos=[101577, 2], node_types=[101577], label=[32], covid=[32], train_mask=[101577], test_mask=[101577], y=[32], name=[32], features=[32, 33], features_names=[32], batch=[101577], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(test_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    data.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfb7d5-9ec0-44b8-8ee1-af2532a61d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train network K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9951d9a-9c8d-49e3-b6ba-449669e900b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b587a5-af10-4600-9dc2-891ad923c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3263ef3c-9a03-4b76-b476-75f3901374e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print K fold model number of samples and number of positive cases\n",
    "\n",
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset)):\n",
    "#     train_subset = dataset.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "#     train_positives = 0\n",
    "#     test_positives = 0\n",
    "#     for step, data in enumerate(train_loader):\n",
    "#         train_positives += np.sum(data.y.numpy())\n",
    "#     for step, data in enumerate(val_loader):\n",
    "#         test_positives += np.sum(data.y.numpy())\n",
    "        \n",
    "#     print(len(train_subset), train_positives, len(val_subset), test_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85ccfab-112f-4ac3-9f25-93fc7ebbbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = '082223_Covid'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_moprh_{condition}\" \n",
    "project_name = f'SF_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d0a76d-a649-497b-a848-0a43a7d860f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "# pools = ['mean', 'max', 'attention', 'attention2']\n",
    "# models = ['GCN', 'GraphConv', 'GAT', 'GINConv', 'SAGEConv']\n",
    "pools = ['mean', 'max', 'attention2']\n",
    "# pools = ['attention2']\n",
    "models = [ 'GAT', 'SAGEConv']\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3908184-60cd-4d11-9bd3-44897ded1790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c541ef1a2304dcba6bdba24c61d3b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_004159-zg9yanth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/zg9yanth' target=\"_blank\">GAT_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/zg9yanth' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/zg9yanth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▃▄▄▅▆▆▇▆▇▇▇▇▇▇▇▇███▇▇█▇▇███████████</td></tr><tr><td>train_auc</td><td>▁▂▁▃▃▄▄▅▆▆▇▇▇▇▇█▇▇██████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▃▃▃▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇▇██▇▇▇█▇▇██▇████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▆▆▅▄▄▃▃▃▂▂▂▃▂▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>███▇██▆▆▄▄▃▂▄▂▃▁▆▇▂▄▃▄▂▃▂▄▄▂▁▁▂▂▂▂▃▂▂▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▃▃▃▆▆▅▆▇▆▆▄▆▆▇▆▅▆▆▅▆▇▇▇▇▇▇▇█▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▄▃▃▃▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█████▇█████</td></tr><tr><td>val_f1</td><td>▂▂▂▂▃▃▃▄▆▆▄▅▇▅▅▁▆▅▇▄▄▅▅▃▅▇▆▇▆▇▇▇▇▇▆▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇█▇▆▆▆▄▃▅▃▂▃▃▅▃▄▃▅▄▃▄▄▃▂▂▄▂▂▁▂▂▂▃▂▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▇▆▅▄▂▁▃▂▃▃▁▄▅▄▂▃▃▃▄▄▂▂▃▃▁▂▄▁▁▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.94558</td></tr><tr><td>train_auc</td><td>0.9879</td></tr><tr><td>train_f1</td><td>0.95266</td></tr><tr><td>train_loss_epoch</td><td>0.36996</td></tr><tr><td>train_loss_step</td><td>0.39443</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.87838</td></tr><tr><td>val_auc</td><td>0.96514</td></tr><tr><td>val_f1</td><td>0.89655</td></tr><tr><td>val_loss_epoch</td><td>0.41607</td></tr><tr><td>val_loss_step</td><td>0.40358</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/zg9yanth' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/zg9yanth</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_004159-zg9yanth\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25c786ae85a4e05bb3aa4497a19785e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_011714-jsu4wr2n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/jsu4wr2n' target=\"_blank\">SAGEConv_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/jsu4wr2n' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/jsu4wr2n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▅▅▆▇▆▆▆▇▇▇▇▇▇▇▇▇█▇██████▇▇█▇█▇█████</td></tr><tr><td>train_auc</td><td>▁▂▄▄▄▅▅▆▇▇▇▇▇▇▇▇██▇▇████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▇████████▇▇█████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▅▅▃▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▇▆▆▇▆▃▅▃▃▅▄▃▃▃▃▃▃▃▂▃▂▂▂▅▄▄▃▂▄▃▁▃▂▂▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▂▃▃▃▄▇█▆▅▇▇▅▆▆▅▆▇▇▇▇▇▇▅▆█▇▇▇█▇▇▇▄▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▄▄▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇█▇█▇████</td></tr><tr><td>val_f1</td><td>▃▃▃▁▃▂▂▆█▆▃▇▇▄▇▆▃▅▆▆▇▆▆▆▃▅█▇▇██▇▇▇▁▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▆▆▅▃▃▂▄▂▂▄▃▂▅▂▃▃▂▂▃▃▅▃▂▃▂▂▁▂▂▃▅▂▁▁▃▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▇▆▅▃▂▇▅▂▃▆▂▄▃▁▄▃▃▄▃▃▃▃▁▄▁▂▂▃▄▅▆▂▁▁▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.89456</td></tr><tr><td>train_auc</td><td>0.93769</td></tr><tr><td>train_f1</td><td>0.90746</td></tr><tr><td>train_loss_epoch</td><td>0.42021</td></tr><tr><td>train_loss_step</td><td>0.44802</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.82432</td></tr><tr><td>val_auc</td><td>0.91464</td></tr><tr><td>val_f1</td><td>0.84884</td></tr><tr><td>val_loss_epoch</td><td>0.49903</td></tr><tr><td>val_loss_step</td><td>0.56472</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/jsu4wr2n' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/jsu4wr2n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_011714-jsu4wr2n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f58ba57b404ae0ad769f15df3190d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_015024-ovsi0fgz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/ovsi0fgz' target=\"_blank\">GAT_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/ovsi0fgz' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/ovsi0fgz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4a1f294fd2417d98b5a0601ff0083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▂▄▄▄▄▄▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▆█▇▇▇▇██▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▃▅▅▆▅▅▆▆▇▇▇▇▇▇▇▇█▇█▇████▇██▇███████▇</td></tr><tr><td>train_f1</td><td>▁▃▃▄▃▅▄▅▄▃▅▅▆▆▆▆▅▆▇▆▇▆▇▇▇▆▆▇▆▇▆▇▇▇██▇█▆▇</td></tr><tr><td>train_loss_epoch</td><td>███▇█▆▆▅▅▆▅▅▃▃▃▃▄▃▂▃▂▂▂▂▂▂▂▂▃▁▂▂▂▂▁▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▆▇▆▆▆█▅▄▅▂▄▄▄▂▃▂▄▄▂▄▁▃▃▄▃▁▂▂▃▃▂▂▂▂▃▃▃▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▃▃▃▃▅▅▆▆▆▃▃▇▇▆▄▅▆▄▇▇▇▆▂▆▅▇▇█▇▇▁▂▆▇█▃▄▆▃▅</td></tr><tr><td>val_auc</td><td>▁▂▄▄▄▅▅▆▆▅▅▇▇▇▇▇▇▇███▇▇███▇█▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>val_f1</td><td>▆▆▆▆▇▇▇▇▇▄▄▇▇▆▄▅▇▄▇▇▇▆▂▇▆▇▇█▇▇▁▂▆██▃▅▇▄▆</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▅▄▃▃▆▆▂▂▃▅▅▃▅▃▃▂▄▇▃▄▂▃▁▃▄█▇▄▂▂▇▆▃▇▅</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▆▅▄▄▆▂▆▃▃▅▃▅▄▅▂▂▃▄█▃▅▂▃▁▃▄▆█▄▁▁█▆▅▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.83333</td></tr><tr><td>train_auc</td><td>0.90263</td></tr><tr><td>train_f1</td><td>0.86119</td></tr><tr><td>train_loss_epoch</td><td>0.46348</td></tr><tr><td>train_loss_step</td><td>0.46346</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.70946</td></tr><tr><td>val_auc</td><td>0.84963</td></tr><tr><td>val_f1</td><td>0.71141</td></tr><tr><td>val_loss_epoch</td><td>0.60594</td></tr><tr><td>val_loss_step</td><td>0.60345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/ovsi0fgz' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/ovsi0fgz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_015024-ovsi0fgz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682993a2325d4ea9b8621dd8be58256e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017450000004221995, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_023210-dx68na5l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dx68na5l' target=\"_blank\">SAGEConv_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dx68na5l' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/dx68na5l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▃▄▄▅▅▆▇▆▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇█████</td></tr><tr><td>train_auc</td><td>▁▃▃▃▄▄▆▅▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇██▇█</td></tr><tr><td>train_f1</td><td>▁▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇█▆▇▇▇▇▇▇▇▇█▇█▇▇██▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▇▆▆▄▅▄▃▄▃▂▂▃▂▂▃▃▃▂▃▂▃▂▂▂▃▁▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▆▆▅▄▅▄▄▂▄▄▃▆▄▄▄▄▂▅▁▁▃▃▃▁▁▃▂▄▄▂▄▂▃▅▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▆▇▆▆▆▃█▅▅█▃▇▄▄▇▆▇▅▅▅▃▆▄▄▆▆▆█▂▆▄▆▂▆▅</td></tr><tr><td>val_auc</td><td>▁▃▆▇▇▇▇█▇▇█▇▇▇███▇▇█▇█▇▇█▇▇█▇█▇▇█▇▇▇█▇▇█</td></tr><tr><td>val_f1</td><td>▆▆▆▆▇▇▇▅▆▅▂▇▅▆▇▁▇▃▄▆▆▇▅▆▄▂▆▄▃▆▅▆█▁▇▅▆▃▆▅</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▅▃▃▄▅▇▁▃▅▂▇▁▆▄▅▅▁▅▅▄▇▄▄▆▃▄▃▃█▃▄▃▆▂▃</td></tr><tr><td>val_loss_step</td><td>▆▆▅▆▅▃▃▅▄▄▅▁▆▃▄▇▅▇▄▅▃▂▅▃▅█▄▃▅▃▄▅▄▅▅▅▅▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.82653</td></tr><tr><td>train_auc</td><td>0.88274</td></tr><tr><td>train_f1</td><td>0.84592</td></tr><tr><td>train_loss_epoch</td><td>0.48141</td></tr><tr><td>train_loss_step</td><td>0.46418</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.7027</td></tr><tr><td>val_auc</td><td>0.83079</td></tr><tr><td>val_f1</td><td>0.71429</td></tr><tr><td>val_loss_epoch</td><td>0.56106</td></tr><tr><td>val_loss_step</td><td>0.46256</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dx68na5l' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/dx68na5l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_023210-dx68na5l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bf3660596f4dc392a516f3acf7be6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_025216-2lrgyv64</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2lrgyv64' target=\"_blank\">GAT_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2lrgyv64' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/2lrgyv64</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2366cd19f07242bba3dad99a784069dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▄▄▅▅▅▆▆▆▇▆▇█▇▇▇█▇███████▇▇██████▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▄▅▅▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▄▄▄▄▅▅▅▆▆▆▇▆▇█▇▇▇█▇███████▇▇██████▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>████▇▆▅▅▄▃▄▃▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▇█▆▇▄▇▄▄▅▂▄▃▃▂▃▄▃▂▃▃▂▁▂▁▁▂▃▄▁▂▂▂▃▁▂▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▄▄▄▆▇▇▇▅▄▇▆▇▇▄▆▇▆▆▆▆▆▆▇▅██▇▇▇█▇▇▂█▇</td></tr><tr><td>val_auc</td><td>▁▂▄▄▄▄▅▆▆▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇█▇████████▅██</td></tr><tr><td>val_f1</td><td>▁▁▁▁▃▃▄▄▅▇▇▇▅▄▇▆▆▆▄▆▇▆▆▆▆▆▆▆▅▇█▇▆▇█▆▇▂█▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▆▆▅▄▃▃▂▃▅▂▃▃▂▆▃▂▂▃▄▂▃▃▃▄▂▁▂▂▂▁▂▂█▁▂</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▆▄█▅▄▆▂▂▃▅▁▃▃▂▃▃▁▃▅▃▅▃▃▇▃▃▂▅▃▂▂▁▃▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.94898</td></tr><tr><td>train_auc</td><td>0.98913</td></tr><tr><td>train_f1</td><td>0.95413</td></tr><tr><td>train_loss_epoch</td><td>0.35995</td></tr><tr><td>train_loss_step</td><td>0.31508</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.86486</td></tr><tr><td>val_auc</td><td>0.94046</td></tr><tr><td>val_f1</td><td>0.89011</td></tr><tr><td>val_loss_epoch</td><td>0.43849</td></tr><tr><td>val_loss_step</td><td>0.45859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2lrgyv64' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/2lrgyv64</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_025216-2lrgyv64\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798eb34ed7b94a0285c8fe52e436c2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_031440-iocduasc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/iocduasc' target=\"_blank\">SAGEConv_2_16_0_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/iocduasc' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/iocduasc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aac9beda7df4d4b9db8bc58bda08c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▄▅▆▅▆▆▆▇▆▇▇▇▇▇▇▆▇▇█▇▇█▇█▇██▇███▇███</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██████████████</td></tr><tr><td>train_f1</td><td>▁▄▅▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇███▇██▇███▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇██▆▅▅▄▄▃▂▃▃▃▂▃▃▂▂▅▄▂▃▂▂▃▂▃▂▄▄▃▃▁▁▃▁▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▂▂▂▂▂▅▄▃▅▅▇▆▆▆▇▅▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▂▂▃▆▆▆▆▇▇▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇████▇█▇██▇██</td></tr><tr><td>val_f1</td><td>▃▃▂▂▁▁▄▃▂▄▄▇▆▆▆▆▅▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▇▆▄▅▅▄▃▂▂▃▄▄▃▂▂▂▃▂▂▂▂▂▂▃▂▂▁▂▃▁▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▆▆▅▄▆▃▅█▅▂▂▄█▃▃▁▄▃▃▃▂▂▃▂▃▂▄▂▇▂▁▁▃▃▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.88776</td></tr><tr><td>train_auc</td><td>0.94531</td></tr><tr><td>train_f1</td><td>0.9049</td></tr><tr><td>train_loss_epoch</td><td>0.42045</td></tr><tr><td>train_loss_step</td><td>0.44497</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.86486</td></tr><tr><td>val_auc</td><td>0.92359</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.45514</td></tr><tr><td>val_loss_step</td><td>0.49135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_0_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/iocduasc' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/iocduasc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_031440-iocduasc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a6abe6cd7e403a903e327e5edb7c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_034154-1wjkaoeu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/1wjkaoeu' target=\"_blank\">GAT_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/1wjkaoeu' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/1wjkaoeu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e424864f5fb44d1db11a79ff7c72884a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▃▄▅▅▆▆▇▇▇▇▇▇▇█▇▇█▇▇▇▇███▇▇█▇███████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▃▅▆▆▇▇▇▇▇▇█▇▇██▇██▇████████████████</td></tr><tr><td>train_f1</td><td>▁▃▃▃▃▃▄▅▅▆▆▇▇▇▇▇▇▇█▇▇█▇▇▇▇███▇▇█▇██████▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▆▅▄▄▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▇█▇▅▇▇▄▄▃▃▃▃▂▂▃▂▂▃▁▂▂▂▂▁▂▃▂▂▃▂▂▃▁▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▂▃▄▅▅▆▆▇█▇▇█▇▆▆▇█▇█▇███████████▆▇▇███</td></tr><tr><td>val_auc</td><td>▁▁▄▄▄▄▅▆▇▇██▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂▂▃▄▅▄▆▆▇▇▇█▆▅▅▇▇▇█▅▇▇▇▇▇█▇█▇█▇▅▇▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▇▅▅▄▄▂▃▂▃▂▂▃▃▂▂▂▁▃▂▂▂▁▂▁▂▁▂▂▁▃▂▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>▇█▇▇▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▃▂▂▃▃▂▁▂▄▂▂▁▃▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.92542</td></tr><tr><td>train_auc</td><td>0.97404</td></tr><tr><td>train_f1</td><td>0.93413</td></tr><tr><td>train_loss_epoch</td><td>0.39169</td></tr><tr><td>train_loss_step</td><td>0.38371</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.93878</td></tr><tr><td>val_auc</td><td>0.97556</td></tr><tr><td>val_f1</td><td>0.94479</td></tr><tr><td>val_loss_epoch</td><td>0.37734</td></tr><tr><td>val_loss_step</td><td>0.3778</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/1wjkaoeu' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/1wjkaoeu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_034154-1wjkaoeu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15ba0c83e35445ba9de4b435cfe0c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_040951-pckgt3g8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/pckgt3g8' target=\"_blank\">SAGEConv_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/pckgt3g8' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/pckgt3g8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee13e2a00c1b49cfb093600fc3d7933b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▃▄▄▄▅▆▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇█▇█▇██▇█</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▄▅▆▆▇▆▇▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█▇█▇▇▇▇▇▇███</td></tr><tr><td>train_f1</td><td>▁▄▅▅▅▆▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇███▇████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▅▅▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇██▆▅▆▄▃▄▅▃▂▃▂▄▂▃▃▁▁▂▃▃▁▂▂▂▃▃▁▄▂▂▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▃▃▄▄▅▅▆▆▇▇▆▇▇▇▆▇██▇▇▇█▇▇██▇▇▇▇████▇██▇</td></tr><tr><td>val_auc</td><td>▁▂▃▃▃▄▆▆▆▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▂▂▂▁▄▅▅▆▆▆▆▆▆▆▅▅▇▇▇▇▇▇▆▇▇▇▆▇▇▇█▇▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▆▅▄▄▃▃▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▁▂▁▃▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▇▇▇▇▅▃▃▅▂▄▃▄▄▄▃▂▃▂▃▂▄▂▂▃▃▄▃▁▄▄▃▃▃▅▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.89153</td></tr><tr><td>train_auc</td><td>0.9485</td></tr><tr><td>train_f1</td><td>0.90805</td></tr><tr><td>train_loss_epoch</td><td>0.42123</td></tr><tr><td>train_loss_step</td><td>0.42665</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.84354</td></tr><tr><td>val_auc</td><td>0.93246</td></tr><tr><td>val_f1</td><td>0.8655</td></tr><tr><td>val_loss_epoch</td><td>0.44943</td></tr><tr><td>val_loss_step</td><td>0.40568</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/pckgt3g8' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/pckgt3g8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_040951-pckgt3g8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7e76c0ec084a32b8834384d093ce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_043109-5tvpazat</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/5tvpazat' target=\"_blank\">GAT_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/5tvpazat' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/5tvpazat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▃▂▃▄▄▅▅▆▆▅▆▆▆▇▆▆▆▆▇▆▇▆▇▇▇▇█▇▇▇█▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▃▃▄▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇██████▇▇████▇██</td></tr><tr><td>train_f1</td><td>▁▂▃▃▃▃▃▄▄▅▅▆▆▅▆▆▆▇▅▆▅▆▇▆▇▆▇▇▇▇█▆▆▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▆▆▅▅▄▄▃▄▄▃▃▃▃▃▄▃▂▃▂▂▂▂▂▂▁▂▃▂▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▇█▇▆▇█▄▆▆▄▃▄▄▃▅▃▄▄▅▃▃▃▄▃▂▃▄▄▄▃▅▅▄▃▄▄▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▃▄▄▄▃▂▂▂▂▃▃▆▆▇▆▆▅▄▃█▇▇▆█▆▇▆▇█▆▇▇▅▆▆</td></tr><tr><td>val_auc</td><td>▁▁▃▄▄▅▅▆▇▆▆▆▆▇█▇▇██████████▇██████▇████▇</td></tr><tr><td>val_f1</td><td>▁▂▂▂▃▃▄▄▃▃▂▂▂▂▃▃▅▆▇▆▆▅▄▃█▇▇▅█▆▇▆▇█▅▇▇▅▆▅</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▇▆▆▅▆▇▇▆▆▆▆▄▃▃▄▃▃▄▅▂▂▂▂▁▂▂▂▂▂▂▂▂▃▄▃</td></tr><tr><td>val_loss_step</td><td>▇█▇▇▇▇▆▆▆▇▆█▆▆▆▅▄▃▄▃▂▅▄▆▂▄▃▃▁▄▃▃▃▂▃▄▄▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.84746</td></tr><tr><td>train_auc</td><td>0.90358</td></tr><tr><td>train_f1</td><td>0.87465</td></tr><tr><td>train_loss_epoch</td><td>0.45693</td></tr><tr><td>train_loss_step</td><td>0.41357</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.79592</td></tr><tr><td>val_auc</td><td>0.91119</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.53643</td></tr><tr><td>val_loss_step</td><td>0.59165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/5tvpazat' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/5tvpazat</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_043109-5tvpazat\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b3f38ef6a24d918bdad5e3823b54b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_045759-goto6ysj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/goto6ysj' target=\"_blank\">SAGEConv_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/goto6ysj' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/goto6ysj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▆▇▇▇▅▇▆▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▃▄▄▅▆▆▆▅▆▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇███████</td></tr><tr><td>train_f1</td><td>▁▅▅▅▅▅▅▅▆▆▆▅▆▆▆▆▅▆▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇██▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▆▆▅▄▄▄▄▄▅▄▄▄▃▃▃▃▂▃▂▂▃▂▂▂▄▂▃▂▂▁▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▇▇███▅▅▃▅▃▅▅▆▅▃▄▅▆▃▂▂▄▅▁▂▂▂▃▂▃▃▃▁▂▂▃▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▆▆▇▆▆▆▅▄▆▃▇▄▇▅▆▇▅▇▆▇▆▆██▆██▇▇█▇▆▇▆▇</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇▇▇▇▇▇▇▆▇▆▇▇█▇▇█▇█▇▇▇███▇██▇▇████▇█</td></tr><tr><td>val_f1</td><td>▅▅▅▅▅▇▆▇▆▅▆▅▃▆▁▆▃▇▅▅▆▄▇▆▆▆▅▇█▆██▇▇██▇▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▅▄▄▄▄▃▃▆▄▆▂▄▃▄▄▂▅▂▃▃▃▄▂▁▂▁▁▂▃▁▁▃▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▆▅▄▅▄▃▅▃▇▃▆▄▃▇▃▃▃▆▂▃▅▃▃▃▄▁▂▂▃▄▁▃▃▃▂▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.83729</td></tr><tr><td>train_auc</td><td>0.90804</td></tr><tr><td>train_f1</td><td>0.86047</td></tr><tr><td>train_loss_epoch</td><td>0.47137</td></tr><tr><td>train_loss_step</td><td>0.56903</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.82993</td></tr><tr><td>val_auc</td><td>0.93414</td></tr><tr><td>val_f1</td><td>0.82759</td></tr><tr><td>val_loss_epoch</td><td>0.50565</td></tr><tr><td>val_loss_step</td><td>0.60856</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/goto6ysj' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/goto6ysj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_045759-goto6ysj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8536e582ac3b48e1b6c45acc086911b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_052427-wdcaw5hq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/wdcaw5hq' target=\"_blank\">GAT_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/wdcaw5hq' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/wdcaw5hq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8eb922460e4555b0300ca167f8bd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇███▇███████▇█▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▄▅▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▃▃▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇███▇███████▇█▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▆▅▄▃▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>████▆▆█▅▅▄▄▄▃▃▄▃▄▃▃▃▂▂▃▃▃▂▄▁▁▁▂▁▁▃▂▄▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▃▄▄▅▃▆█▇█████▄▇▆▇▇▇▇▅▅▆▅▅▆▄▅▇▄▃▄▆▄▅</td></tr><tr><td>val_auc</td><td>▁▁▃▃▄▄▅▆▇▇████████▇██████▇▇▇▇▇█▇▇██▇▇█▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▃▃▃▃▅▃▅█▇█▇▇██▄▇▅▆▇▇▇▅▄▅▅▄▆▃▄▇▄▃▄▆▃▄</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▇▆▇▅▆▄▁▂▁▂▁▁▂▅▂▃▂▂▂▂▄▄▃▅▄▄▆▅▂▄█▅▃▆▆</td></tr><tr><td>val_loss_step</td><td>▇█▇▇▆▇▇▃▂▁▄▁▃▁▅▁▁▄▃▂▃▃▂▂▄▄▄▃▆▆▄▇▄▂▅▆▃▇▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.96271</td></tr><tr><td>train_auc</td><td>0.99341</td></tr><tr><td>train_f1</td><td>0.96812</td></tr><tr><td>train_loss_epoch</td><td>0.34956</td></tr><tr><td>train_loss_step</td><td>0.35194</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.72789</td></tr><tr><td>val_auc</td><td>0.92192</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.62112</td></tr><tr><td>val_loss_step</td><td>0.72857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/wdcaw5hq' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/wdcaw5hq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_052427-wdcaw5hq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f279419a901d4620a81cffbdb8b8d07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_054522-tt2vr3tf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/tt2vr3tf' target=\"_blank\">SAGEConv_2_16_1_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/tt2vr3tf' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/tt2vr3tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▃▄▅▅▅▅▅▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇███▇▇████████▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▃▄▄▅▄▅▄▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇███▇▇████████▇█</td></tr><tr><td>train_f1</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▆▇▇▇█▇▆████▇██████████████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▇▅▅▄▆██▆▅▄▇▂▃▄▃▃▃▅▃▂▂▂▃▁▁▃▂▄▁▁▁▅▃▃▁▃▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▃▄▄▅▄▅▄▄▅▅▅▆▇▇▇█▇▇█▇▇▇██▇███▇█▇█████▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▃▃▃▄▄▅▄▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>val_f1</td><td>▃▃▁▄▃▄▃▅▅▃▅▅▄▆▇▇▇▇▇▇█▇▇▇██▇██▇▇█▇█████▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▆▆▇▅▇▅▄▅▆▄▃▃▂▃▃▃▂▂▂▃▂▂▁▂▂▂▃▂▂▂▃▁▃▂▂▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▃▇▅█▅▃█▅▅▄▂▃▃▅▆▃▁▁▃▁▄▂▁▃▃▁▃▃▂▁▂▁▂▄▁▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.87458</td></tr><tr><td>train_auc</td><td>0.92741</td></tr><tr><td>train_f1</td><td>0.89636</td></tr><tr><td>train_loss_epoch</td><td>0.43489</td></tr><tr><td>train_loss_step</td><td>0.41894</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.85034</td></tr><tr><td>val_auc</td><td>0.92285</td></tr><tr><td>val_f1</td><td>0.86747</td></tr><tr><td>val_loss_epoch</td><td>0.48672</td></tr><tr><td>val_loss_step</td><td>0.57638</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_1_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/tt2vr3tf' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/tt2vr3tf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_054522-tt2vr3tf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828a13f918d442efb13c9ff95abe5cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_060533-qxh48477</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/qxh48477' target=\"_blank\">GAT_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/qxh48477' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/qxh48477</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2058764373aa4070ad3e36f2915b60be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▄▄▅▅▆▆▇▇▇▇▇▇▇▆▆▇▇▇▇███▇▇███████▇█▇██</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▄▅▆▇▇▇▇█▇███▇██████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▆▆▇▇▇▇███▇▇██████▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▆▅▅▄▄▃▃▂▃▂▂▂▄▃▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▄▆▆▄▃▄▂▃▃▂▂▂▆▅▃▂▂▂▂▂▂▁▃▂▂▂▂▂▂▂▁▂▄▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▂▄▃▆▆█▆▇█▅▆▆█▆▅▇▇█▆▆▆▇▄▄█▆▅▇▆▆▄▄▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▂▂▃▃▅▆▆▇▇▆▇▇▇█▇█▇▇▇▇███▇█▇██▇▇██▇▇████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▂▃▂▅▅▇▆▇█▅▅▆▇▆▄▇▇█▆▅▅▇▄▄█▅▄▇▆▆▃▃▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>████▇▆▅▇▄▄▁▄▂▁▄▄▃▁▄▅▃▄▂▄▂▅▂▅▆▂▄▄▃▄▄▆▆▃▂▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▆█▄▆▄▃▃▄▂▃▃▃▅▁▃▂▂▂▃▃▂█▂▃▆▆▅▄▃▇▄▁▆▆▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.95254</td></tr><tr><td>train_auc</td><td>0.97619</td></tr><tr><td>train_f1</td><td>0.95758</td></tr><tr><td>train_loss_epoch</td><td>0.37418</td></tr><tr><td>train_loss_step</td><td>0.40516</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.79592</td></tr><tr><td>val_auc</td><td>0.93347</td></tr><tr><td>val_f1</td><td>0.84211</td></tr><tr><td>val_loss_epoch</td><td>0.51571</td></tr><tr><td>val_loss_step</td><td>0.56359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/qxh48477' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/qxh48477</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_060533-qxh48477\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccaf919d6234054aceb3ea1e5340e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_064354-hbu379e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/hbu379e4' target=\"_blank\">SAGEConv_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/hbu379e4' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/hbu379e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▄▄▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▆▆▇██▇██▇███▇▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▄▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇████████▇████</td></tr><tr><td>train_f1</td><td>▁▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇▆▇██▇██████▇▇██▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▆▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▃▄▂▂▂▂▂▁▂▁▁▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇██▇█▅▄▄▃▃▃▂▃▂▆▂▂▂▄▃▂▅▄▂▁▄▂▂▃▂▂▁▃▁▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▂▂▃▃▅▅▅▄▅▆▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▂▂▂▃▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇████▇████</td></tr><tr><td>val_f1</td><td>▂▂▂▁▃▂▃▃▄▂▃▄▄▃▆▅▆▆▆▆▆▇▆▇▇▆▆▇▆▇▇▆▇▇▇▆▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▆▄▅▃▄▄▃▄▄▃▃▄▂▃▃▂▂▃▂▂▂▃▃▃▂▂▂▂▂▂▂▂▂▃▁</td></tr><tr><td>val_loss_step</td><td>████▇▆▄▅▄▃▅▁▄▅▄▄▄▂▅▅▅▃▄▃▃▃▅▃▆▄▃▃▃▂▃▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.87119</td></tr><tr><td>train_auc</td><td>0.9466</td></tr><tr><td>train_f1</td><td>0.89385</td></tr><tr><td>train_loss_epoch</td><td>0.42634</td></tr><tr><td>train_loss_step</td><td>0.37335</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.91837</td></tr><tr><td>val_auc</td><td>0.96626</td></tr><tr><td>val_f1</td><td>0.93333</td></tr><tr><td>val_loss_epoch</td><td>0.38945</td></tr><tr><td>val_loss_step</td><td>0.38224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/hbu379e4' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/hbu379e4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_064354-hbu379e4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5d7022aa3c4a6f974e80b929636677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_070352-xie5i1hp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/xie5i1hp' target=\"_blank\">GAT_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/xie5i1hp' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/xie5i1hp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▃▄▄▅▅▅▆▅▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇█▇▇▇▇▇███</td></tr><tr><td>train_auc</td><td>▁▂▂▃▃▄▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███▇▇█████████▇██</td></tr><tr><td>train_f1</td><td>▁▃▃▄▃▄▄▅▅▅▆▅▇▆▇▇▆▆▇▇▇▇▇▇▇█▇▇█▇▇█▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▆▅▅▅▃▄▃▃▃▃▃▃▂▂▃▃▃▂▂▂▂▂▁▂▃▁▂▂▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▆▅▆▆▆▅▅▄▅▄▂▃▂▄▃▃▃▃▃▄▃▁▃▃▂▃▁▂▁▄▂▂▂▂▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▂▂▂▂▃▅▅▄▄▃▇▇█▆█▄█▁██▇██▇██▇▆▇▇▆▅▃▅▆▇▆▇▆▅</td></tr><tr><td>val_auc</td><td>▁▂▂▄▄▄▅▆▇▇▇██████▇██▇█▇███▇█▇▇▇▇▇▇▇█▇▇▆▆</td></tr><tr><td>val_f1</td><td>▆▆▆▆▆▇▇▆▆▆▇██▆█▄█▁██▇█████▇▇▇▇▇▆▄▆▇█▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>████▇▆▅▅▅▅▅▄▄▄▃▄▂▇▁▁▂▁▂▁▂▂▂▂▂▁▂▃▅▄▂▁▂▂▃▄</td></tr><tr><td>val_loss_step</td><td>▆▇▆▇▆▆▄▅▄▄▅▃▄▄▃▄▂█▂▂▃▃▅▂▂▅▂▄▂▃▁▃▄▅▂▁▁▂▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.86102</td></tr><tr><td>train_auc</td><td>0.92557</td></tr><tr><td>train_f1</td><td>0.88252</td></tr><tr><td>train_loss_epoch</td><td>0.45266</td></tr><tr><td>train_loss_step</td><td>0.50861</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.72109</td></tr><tr><td>val_auc</td><td>0.82634</td></tr><tr><td>val_f1</td><td>0.74534</td></tr><tr><td>val_loss_epoch</td><td>0.58452</td></tr><tr><td>val_loss_step</td><td>0.64695</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/xie5i1hp' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/xie5i1hp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_070352-xie5i1hp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dabce27f17490cbe7dde28dfab05ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_072503-dzgbx6qd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dzgbx6qd' target=\"_blank\">SAGEConv_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dzgbx6qd' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/dzgbx6qd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▃▄▅▅▅▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇▇██▇█▇████▇████</td></tr><tr><td>train_auc</td><td>▁▂▁▂▄▂▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇█████████</td></tr><tr><td>train_f1</td><td>▁▅▆▆▆▅▆▆▆▆▆▇▆▆▇▆▇▇▇▆▇▇▇▇▇▇██▇███████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▇▆▅▄▄▄▄▄▄▃▄▂▃▃▄▃▃▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇███▇▆▆▆▅▆▆▅▅▆▅▃▆▅▃▄▃▄▄▃▄▂▄▄▃▃▃▁▄▃▁▄▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▄▄▆▆▂▅▅▅▃▅▂▃▄▄▅▄▄▅▄▇▅▆▄▆▆▅▅█▅▇▇▇▆▆</td></tr><tr><td>val_auc</td><td>▁▂▄▅▆▆▇▇▇▇▇▇▇▇▇▇█▆█▇▇▇▇█▇█▇█▇▇▇▇▇█▇▇██▇▇</td></tr><tr><td>val_f1</td><td>▅▅▅▅▅▆▆▄▆▆▁▅▅▅▂▆▁▄▃▄▅▄▄▅▄▇▆▆▅▆▆▆▅█▅▇▇▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▇▅▆▃▄▇▄▄▃▆▄▇▇▄▄▅▃▄▂▆▃▃▁▅▄▄▃▄▂▅▂▂▃▃▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▇▆▃▅▅▅▆▅▅▆▅▄▆█▆▄▇▅▄▅▅▅▄▆▆▄█▆▅▆▅▅▂▁▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.82373</td></tr><tr><td>train_auc</td><td>0.8936</td></tr><tr><td>train_f1</td><td>0.85635</td></tr><tr><td>train_loss_epoch</td><td>0.48316</td></tr><tr><td>train_loss_step</td><td>0.46932</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.7551</td></tr><tr><td>val_auc</td><td>0.82587</td></tr><tr><td>val_f1</td><td>0.78824</td></tr><tr><td>val_loss_epoch</td><td>0.54934</td></tr><tr><td>val_loss_step</td><td>0.5678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/dzgbx6qd' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/dzgbx6qd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_072503-dzgbx6qd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008174435d8d4f48983b78793899cc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_074533-74ud8shr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/74ud8shr' target=\"_blank\">GAT_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/74ud8shr' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/74ud8shr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GAT              | 4.8 K \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a09e6669dd4492b374927dddf627a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇████████▇▇█████</td></tr><tr><td>train_auc</td><td>▂▁▂▃▄▅▅▅▆▇▇▇▇█▇▇█████████████████▇██████</td></tr><tr><td>train_f1</td><td>▁▂▃▃▄▄▄▄▅▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇████████▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▆▅▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▂▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▅▆▆▃▄▃▃▄▂▂▂▃▃▂▂▂▄▁▂▁▂▂▂▂▁▂▁▁▂▄▃▃▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▂▁▁▃▂▃▂▄▅▅▄▅█▄▂▆▅▂▅▆▃▆▄▅▅▃▃▄▅▃▅▃▄▇█▅▄▃▅▅</td></tr><tr><td>val_auc</td><td>▁▃▃▂▄▄▆▇▇▆▆▆▇▆▄▆▆▄▇▇▅▆▅▆▅▄▄▅▆▅▆▅▆▇█▆▅▅▆▅</td></tr><tr><td>val_f1</td><td>▂▁▁▂▂▃▂▄▅▄▃▄█▃▂▅▄▁▅▅▂▆▄▄▄▂▂▃▄▂▄▂▃▆█▅▃▂▅▃</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇▅▇▇▅▅▅▆▃▆█▄▅▇▃▄▆▅▅▄▅▇▅▅▅█▅▇▆▃▁▅▆▇▄▆</td></tr><tr><td>val_loss_step</td><td>▆▇▆▆▅▆▅▆▅▄▆█▁▃█▂▂▄▄▆▆▃▅▄▃▆▆▇▄▄▅▆▅▃▂▂▄▆▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.95932</td></tr><tr><td>train_auc</td><td>0.98503</td></tr><tr><td>train_f1</td><td>0.96364</td></tr><tr><td>train_loss_epoch</td><td>0.36159</td></tr><tr><td>train_loss_step</td><td>0.40322</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.70068</td></tr><tr><td>val_auc</td><td>0.78908</td></tr><tr><td>val_f1</td><td>0.77551</td></tr><tr><td>val_loss_epoch</td><td>0.61396</td></tr><tr><td>val_loss_step</td><td>0.62401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/74ud8shr' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/74ud8shr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_074533-74ud8shr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7d495043594e49938c724643caeb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\11_snowflakes\\notebooks\\wandb\\run-20230823_084213-2hidm3o1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2hidm3o1' target=\"_blank\">SAGEConv_2_16_2_moprh</a></strong> to <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/SF_082223_Covid' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2hidm3o1' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/2hidm3o1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Global seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 42\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "   | Name           | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0  | feat_embedding | Sequential       | 816   \n",
      "1  | model          | GNNModel         | 736   \n",
      "2  | head           | Sequential       | 562   \n",
      "3  | fnn_layer      | Linear           | 272   \n",
      "4  | selu           | SELU             | 0     \n",
      "5  | loss_module    | CrossEntropyLoss | 0     \n",
      "6  | train_acc      | BinaryAccuracy   | 0     \n",
      "7  | train_auroc    | BinaryAUROC      | 0     \n",
      "8  | train_f1       | BinaryF1Score    | 0     \n",
      "9  | valid_acc      | BinaryAccuracy   | 0     \n",
      "10 | valid_auroc    | BinaryAUROC      | 0     \n",
      "11 | valid_f1       | BinaryF1Score    | 0     \n",
      "12 | pool           | Attention_module | 281   \n",
      "-----------------------------------------------------\n",
      "2.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Morphological features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d930c8d7fb743a1b24f3064d8c4105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▄▄▄▅▅▅▆▇▆▇▇▆▇▇▆▇▇▇▇██▇██▇████▇▇▆▇█▇██</td></tr><tr><td>train_auc</td><td>▂▁▃▄▄▄▅▅▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇███████▇▇█████</td></tr><tr><td>train_f1</td><td>▁▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇██▇██▇████▇▇▆▇████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▆▆▅▅▅▄▃▃▃▂▃▂▃▄▃▂▂▃▂▂▂▂▂▂▁▂▁▁▂▃▄▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▇▄▆▇█▄▆▄▄▄▃▃▃▃▇▂▂▂▅▂▃▄▂▂▄▁▄▂▃▄▅▇▅▃▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▂▂▂▃▂▃▃▂▄▃▄▃▄▃▅▃▅▄▆▄▆▄▆▄▇▇▅▇▅█▅█▅</td></tr><tr><td>val_acc</td><td>▂▁▁▂▂▃▃▅▅▅▆▆▇▇▆▇█▇▇▇▇▆▇▇▇▇▇▇██▇▇▇▅▇█▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▂▂▃▃▄▄▅▆▇▇▇▇▆▇▇▇▇▇▇▇▇██████████▆▇██▇▇▇</td></tr><tr><td>val_f1</td><td>▃▄▁▃▂▃▃▆▆▅▆▆▇▆▆▇█▆▇▇▇▇▇▇▇█▇▇███▇█▆▇██▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▆▇▆▄▄▃▂▃▂▂▃▂▂▄▂▃▃▃▂▂▁▁▃▃▁▁▂▁▂▄▂▂▁▂▃▃</td></tr><tr><td>val_loss_step</td><td>█▇▇▇██▆▅▅▆▄▃▃▄▃▅▄▇▄▃▄▆▃▄▄▂▅▄▂▂▂▄▃▅▅▄▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.88475</td></tr><tr><td>train_auc</td><td>0.94024</td></tr><tr><td>train_f1</td><td>0.89881</td></tr><tr><td>train_loss_epoch</td><td>0.41964</td></tr><tr><td>train_loss_step</td><td>0.43179</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.78912</td></tr><tr><td>val_auc</td><td>0.88039</td></tr><tr><td>val_f1</td><td>0.84729</td></tr><tr><td>val_loss_epoch</td><td>0.50501</td></tr><tr><td>val_loss_step</td><td>0.4417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAGEConv_2_16_2_moprh</strong> at: <a href='https://wandb.ai/thoomas/SF_082223_Covid/runs/2hidm3o1' target=\"_blank\">https://wandb.ai/thoomas/SF_082223_Covid/runs/2hidm3o1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230823_084213-2hidm3o1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print K fold model number of samples and number of positive cases\n",
    "\n",
    "k_folds = 3\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fusion_mode = 'concat'\n",
    "# fusion_mode = 'concat'\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset)):\n",
    "    train_subset = dataset.index_select(train_ids.tolist())\n",
    "    val_subset = dataset.index_select(valid_ids.tolist())\n",
    "    \n",
    "    for pool in pools:\n",
    "        for model in models:\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "            CHECKPOINT_PATH = checkpoint_folder / f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}_moprh_{fusion_mode}' / pool\n",
    "            CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Skip already trained kfold and pool\n",
    "            checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "            if checkpoint.exists():\n",
    "                print(checkpoint)\n",
    "                continue\n",
    "\n",
    "            # Run training\n",
    "            run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}_moprh', \n",
    "                            group=f'{model}_{pool}_moprh_{fusion_mode}')\n",
    "            graph.train_graph_classifier_kfold(model, \n",
    "                                                 train_subset, \n",
    "                                                 val_subset, \n",
    "                                                 dataset, \n",
    "                                                 CHECKPOINT_PATH, \n",
    "                                                 AVAIL_GPUS, \n",
    "                                                 hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                 num_layers=NUM_LAYERS, \n",
    "                                                 epochs=epochs,\n",
    "                                                 morph=True,\n",
    "                                                 fusion = fusion_mode,\n",
    "                                                 graph_pooling=pool)\n",
    "            run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc615dfc-7b37-4861-b20a-9cc44d845103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
