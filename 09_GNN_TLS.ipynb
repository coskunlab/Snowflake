{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "#import napari\n",
    "import tifffile as tiff\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_dir = (Path().cwd().parents[0]).absolute()\n",
    "p_dir = 'Y:\\\\coskun-lab\\\\Efe and Nishkala\\\\SnowflakePipeline'\n",
    "\n",
    "module_path = str(p_dir + \"\\\\src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = (Path().cwd().parents[0] / 'data').absolute()\n",
    "data_dir = (p_dir + '\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "import networkx as nx\n",
    "import lightning.pytorch as pl\n",
    "import torch.utils.data as data\n",
    "\n",
    "process_path = p_dir + '\\\\Data' + '\\\\torch_graph_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "name = 'TLS'\n",
    "\n",
    "# Crate dataset\n",
    "dataset = graph.GraphDataset(process_path + '\\\\' + name, process_path + '\\\\' + name + '\\\\' + 'info.csv', 2, y_name='y')\n",
    "\n",
    "train_set, val_set, test_set = graph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(34):\n",
      "======================\n",
      "Number of graphs: 34\n",
      "Number of features: 58\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.nc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 17, val set: 13, test set: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, val set: {len(test_set)}, test set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 13\n",
      "DataBatch(x=[9951, 58], edge_index=[2, 44722], pos=[9951, 2], y=[13], edge_attr=[44722, 2], name=[13], batch=[9951], ptr=[14])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(test_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    data.y\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'SF_15082024_tls_snowflake_pos'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [0]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 3\n",
    "HIDDEN_CHANNELS = 32\n",
    "pools = ['mean', 'max', 'attention', 'attention2']\n",
    "# pools = ['attention2']\n",
    "models = ['GAT', 'GINConv']\n",
    "#models = ['GINConv']\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [data.y.item() for data in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194041-qygvgps7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qygvgps7' target=\"_blank\">GAT_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qygvgps7' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qygvgps7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\mean\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\mean\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.7143)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▅▅▆▆▇▇▇███████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▅██████▅▅▅▅███████████▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▅▅▅███▅▅▅▅▅▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_f1</td><td>▁▆██████▆▆▆▆███████████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▄▂▂▂▃▇█▆▅▂▁▁▁▁▁▁▁▁▂▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▂▂▂▃▇█▆▅▂▁▁▁▁▁▁▁▁▂▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.71429</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31384</td></tr><tr><td>train_loss_step</td><td>0.31384</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.64227</td></tr><tr><td>val_loss_step</td><td>0.64227</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qygvgps7' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qygvgps7</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194041-qygvgps7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194110-eltezlwp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/eltezlwp' target=\"_blank\">GINConv_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/eltezlwp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/eltezlwp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\mean\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\mean\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.6667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▂▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▆▆▆▆▆▆▆▆████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>▅▅▅▅███▅▅████████▅▅▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅████</td></tr><tr><td>val_f1</td><td>▁▁▆▇▇▇▇▇▇▇▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▅▄▃▃▃▃▃▂▃▁▁▁▁▂▃▃▃▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂</td></tr><tr><td>val_loss_step</td><td>██▅▄▃▃▃▃▃▂▃▁▁▁▁▂▃▃▃▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.66667</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31344</td></tr><tr><td>train_loss_step</td><td>0.31344</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.46886</td></tr><tr><td>val_loss_step</td><td>0.46886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/eltezlwp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/eltezlwp</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194110-eltezlwp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194140-8szdfwpd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8szdfwpd' target=\"_blank\">GAT_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8szdfwpd' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8szdfwpd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\max\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\max\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.5333), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▂▃▃▃▄▆▅▅▅▆▇▅▆▆█▆▄▇▆▆▆▆▆▆▆▆▃██▆▇▆▆▇▇▆</td></tr><tr><td>train_auc</td><td>▁▁▄▄▃▅▃▃▆▅▅▆▅▅█▆█▇██▄█▇▆▇▇▇▆▇▇▅██▇█▇▇███</td></tr><tr><td>train_f1</td><td>▃▃▃▃▁▃▅▅▅▇▆▆▅▆▇▅▇▇█▇▅▇▇▆▆▇▆▇▆▆▄██▇▇▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇█▆▆▇▅▄▅▄▄▄▂▄▂▃▁▂▆▃▃▄▄▂▄▃▃▃▆▁▁▃▂▄▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▃▅▂▃▃▁▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▅▁▅████████████████████████████████████</td></tr><tr><td>val_auc</td><td>█▆▆███████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▃▃▁▃████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.53333</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.88889</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>0.88889</td></tr><tr><td>train_loss_epoch</td><td>0.39735</td></tr><tr><td>train_loss_step</td><td>0.39735</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.0</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.51713</td></tr><tr><td>val_loss_step</td><td>0.51713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8szdfwpd' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8szdfwpd</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194140-8szdfwpd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194209-c1vkznbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/c1vkznbg' target=\"_blank\">GINConv_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/c1vkznbg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/c1vkznbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\max\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\max\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.7692)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▇▇██▇█████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇██▇█████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▅▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆███████▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>█▁▁▁████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▆▆▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇███████▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▂▂▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▂▂▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.76923</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31332</td></tr><tr><td>train_loss_step</td><td>0.31332</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.47556</td></tr><tr><td>val_loss_step</td><td>0.47556</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/c1vkznbg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/c1vkznbg</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194209-c1vkznbg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194239-ooemarul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/ooemarul' target=\"_blank\">GAT_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/ooemarul' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/ooemarul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\attention\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\attention\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.4667), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▃▁▂▅▄▄▄▄▅▄▃▇█▄▆▆▄▇▇▆▄▆▅▆▆▇▇▆▆▇▇▄▆▅▅▅▇▆▇</td></tr><tr><td>train_auc</td><td>▂▅▁▃▄▃▄▃▇▅▃▃▅█▆▆▇▅▆▆█▇▇▆▆▆▇▇▇▇██▆▇▇▇██▆█</td></tr><tr><td>train_f1</td><td>▂▃▁▁▆▅▅▆▅▆▅▄▇█▆▇▇▆▇▇▆▆▆▆▇▇▇▇▇▆▇▇▅▇▆▆▆█▆▇</td></tr><tr><td>train_loss_epoch</td><td>▇▇█▇▆▇▆▆▄▄▆▇▃▁▅▄▃▆▃▃▂▅▃▄▃▄▃▃▃▄▂▂▄▃▄▃▄▂▃▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▅▃▄▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█</td></tr><tr><td>val_f1</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▇██▇▅▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▇██▇▅▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.46667</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.88889</td></tr><tr><td>train_auc</td><td>0.975</td></tr><tr><td>train_f1</td><td>0.875</td></tr><tr><td>train_loss_epoch</td><td>0.42448</td></tr><tr><td>train_loss_step</td><td>0.42448</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.51507</td></tr><tr><td>val_loss_step</td><td>0.51507</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/ooemarul' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/ooemarul</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194239-ooemarul\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194307-s53cvgm3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/s53cvgm3' target=\"_blank\">GINConv_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/s53cvgm3' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/s53cvgm3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\attention\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\attention\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.6667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇███▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▆▇▇███████▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▁▇█████████▇██████████▇█████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▂▁▁▁▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▁▇▇▇▇▇█▇▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▅▆█▆▅▃▅▅▅▃▃▃▃▁▃▃▃▃▅▆▅▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▆▆▆▆▆████████▆▆▃▃▃▃▃▁▃▃▃▃▃▃▃▅▆▆▆▃▃▃▃▃▃▃▃</td></tr><tr><td>val_f1</td><td>▁▁▆▆▇█▇▆▄▆▆▆▄▄▄▄▃▅▅▅▅▆▇▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▃▂▁▂▄▇▄▄▄▆▇▇▇██▇▇▆▅▂▄▅▆▅▅▅▃▄▄▄▅▅▅▅▅▅▅</td></tr><tr><td>val_loss_step</td><td>▅▅▃▃▂▁▂▄▇▄▄▄▆▇▇▇██▇▇▆▅▂▄▅▆▅▅▅▃▄▄▄▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.66667</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.925</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.36886</td></tr><tr><td>train_loss_step</td><td>0.36886</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.72936</td></tr><tr><td>val_loss_step</td><td>0.72936</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/s53cvgm3' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/s53cvgm3</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194307-s53cvgm3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194338-zq8u60xl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/zq8u60xl' target=\"_blank\">GAT_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/zq8u60xl' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/zq8u60xl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\attention2\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_0\\attention2\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.4545), 'test_auc': tensor(0.5333), 'test_f1': tensor(0.4000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▆▅▆▇▇█████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▇▆▇███████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆▆▆▆▆▆█▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆████▆▆▆▆▅▅</td></tr><tr><td>val_auc</td><td>▁▃▅▅▅▆▆▆████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▃▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇▇▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▅▄▄▄▄▃▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▂▃▃▄▅▅</td></tr><tr><td>val_loss_step</td><td>█▇▅▅▄▄▄▄▃▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▂▃▃▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.45455</td></tr><tr><td>test_auc</td><td>0.53333</td></tr><tr><td>test_f1</td><td>0.4</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31363</td></tr><tr><td>train_loss_step</td><td>0.31363</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.58776</td></tr><tr><td>val_loss_step</td><td>0.58776</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/zq8u60xl' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/zq8u60xl</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194338-zq8u60xl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194407-78pwieg6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/78pwieg6' target=\"_blank\">GINConv_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/78pwieg6' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/78pwieg6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\attention2\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_0\\attention2\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8333), 'test_f1': tensor(0.7692)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇█▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆█▆▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅</td></tr><tr><td>val_auc</td><td>▁▁████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▇▇█▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▆▂▂▁▂▃▄▅▅▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>█▆▂▂▁▂▃▄▅▅▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.83333</td></tr><tr><td>test_f1</td><td>0.76923</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31334</td></tr><tr><td>train_loss_step</td><td>0.31334</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.75</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.55429</td></tr><tr><td>val_loss_step</td><td>0.55429</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/78pwieg6' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/78pwieg6</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194407-78pwieg6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194435-i0zncig8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/i0zncig8' target=\"_blank\">GAT_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/i0zncig8' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/i0zncig8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\mean\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\mean\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8667), 'test_f1': tensor(0.8000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▂▃▅▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇█</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▂▃▅▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.86667</td></tr><tr><td>test_f1</td><td>0.8</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31355</td></tr><tr><td>train_loss_step</td><td>0.31355</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.73999</td></tr><tr><td>val_loss_step</td><td>0.73999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/i0zncig8' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/i0zncig8</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194435-i0zncig8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194503-nbvluoqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/nbvluoqg' target=\"_blank\">GINConv_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/nbvluoqg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/nbvluoqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\mean\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\mean\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.7000), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁███████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▄▁███▄▄▄▄▄▄▄▄▄▄▄████████████████████████</td></tr><tr><td>val_auc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▄███▆▆▆▆▆▆▆▆▆▆▆████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▃▃▁▁▄▅▇███▇▇▇▇▇▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▃▃▁▁▄▅▇███▇▇▇▇▇▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.7</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31331</td></tr><tr><td>train_loss_step</td><td>0.31331</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.0</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.71427</td></tr><tr><td>val_loss_step</td><td>0.71427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/nbvluoqg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/nbvluoqg</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194503-nbvluoqg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194530-tet8t407</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tet8t407' target=\"_blank\">GAT_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tet8t407' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tet8t407</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\max\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\max\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.3333), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▁▄▂▄▅▃▅▇▆▆▇▇▅▅▇█▇▅█▆▇▇▅▇▇█▇▇▇▆▆▇█▇█████</td></tr><tr><td>train_auc</td><td>▃▅▄▁▇▄▅▇█▇███▆▇██▇▅█▇███████████████████</td></tr><tr><td>train_f1</td><td>▄▁▄▄▅▅▃▅▇▇▇▇▇▅▆▇█▇▅█▇▇▇▆▇▇█▇▇▇▆▇▇█▇█████</td></tr><tr><td>train_loss_epoch</td><td>▆▆▇█▅▅▆▄▂▃▂▂▂▅▄▂▁▂▄▁▃▂▂▄▂▂▁▂▂▂▃▂▂▁▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▃▂▃▄▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>██▁███▁▁▁███▁▁▁▁▁▁███████▁▁▁▁███████████</td></tr><tr><td>val_auc</td><td>████▁▁██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▅▅▁███▁▁▁███▁▁▁▁▁▁███████▁▁▁▁███████████</td></tr><tr><td>val_loss_epoch</td><td>▁▂▂▁▁▂▂▃▃▃▃▃▅▆▅▅▅▄▃▂▁▁▁▁▂▄██▆▃▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>▁▂▂▁▁▂▂▃▃▃▃▃▅▆▅▅▅▄▃▂▁▁▁▁▂▄██▆▃▁▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.33333</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.33059</td></tr><tr><td>train_loss_step</td><td>0.33059</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.33333</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.7286</td></tr><tr><td>val_loss_step</td><td>0.7286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tet8t407' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tet8t407</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194530-tet8t407\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194559-k0syuvia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/k0syuvia' target=\"_blank\">GINConv_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/k0syuvia' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/k0syuvia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\max\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\max\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.6333), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▃▃▁▃▆█████▆▆▆▆▃▃▃▃▆▆▆▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_auc</td><td>▁▁▁▁██████▆▆▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▃</td></tr><tr><td>val_f1</td><td>▁▄▄▆▇█████▆▆▆▆▄▄▄▄▅▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▇▇█▅▃▂▁▁▁▁▃▃▄▄▅▆▇▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆</td></tr><tr><td>val_loss_step</td><td>▇▇█▅▃▂▁▁▁▁▃▃▄▄▅▆▇▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.63333</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31329</td></tr><tr><td>train_loss_step</td><td>0.31329</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.33333</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.87571</td></tr><tr><td>val_loss_step</td><td>0.87571</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/k0syuvia' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/k0syuvia</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194559-k0syuvia\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194627-6ez25uf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/6ez25uf4' target=\"_blank\">GAT_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/6ez25uf4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/6ez25uf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\attention\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\attention\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▃▁▄▅▄▆▅▇▇▇▇▅▅▆▇▇▆▅▆▅▆▇▆▇▇▇▇█▇▇█▇▇█▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▁▆▆▆▇▅█▇█▆▆▇████▇▇▇▇██▇███████████████</td></tr><tr><td>train_f1</td><td>▂▂▁▆▆▅▇▅█▇█▇▆▆▇██▆▆▇▆▇█▆▇█▇████████▇████</td></tr><tr><td>train_loss_epoch</td><td>▇██▆▆▅▄▆▂▃▂▃▅▄▃▂▂▃▅▄▄▄▂▄▃▂▃▂▁▂▂▁▂▂▁▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▃▅▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅████▅▅▅▅▅▅████████▅▅▅█████████████████</td></tr><tr><td>val_f1</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▁▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_step</td><td>█▆▃▁▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.98765</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.36966</td></tr><tr><td>train_loss_step</td><td>0.36966</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.71287</td></tr><tr><td>val_loss_step</td><td>0.71287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/6ez25uf4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/6ez25uf4</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194627-6ez25uf4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194656-5coem8oi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/5coem8oi' target=\"_blank\">GINConv_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/5coem8oi' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/5coem8oi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\attention\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\attention\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.9333), 'test_f1': tensor(0.8333)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▆██▇██████████▇███████████████████████</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁▁▁█▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃█████▆▆▆▆▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>█▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄█████▆▆▆▆▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▂█▇▇▇▇▇▇▇▇▇▇▇▇▇▁▁▁▁▁▃▅▅▅▅▆▇████████████</td></tr><tr><td>val_loss_step</td><td>▁▂█▇▇▇▇▇▇▇▇▇▇▇▇▇▁▁▁▁▁▃▅▅▅▅▆▇████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.93333</td></tr><tr><td>test_f1</td><td>0.83333</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31327</td></tr><tr><td>train_loss_step</td><td>0.31327</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.0</td></tr><tr><td>val_auc</td><td>0.0</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>1.1563</td></tr><tr><td>val_loss_step</td><td>1.1563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/5coem8oi' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/5coem8oi</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194656-5coem8oi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194723-4dyhmjw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/4dyhmjw1' target=\"_blank\">GAT_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/4dyhmjw1' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/4dyhmjw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\attention2\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_1\\attention2\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.9333), 'test_f1': tensor(0.8333)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇█▇▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▅▇▇█▇▆████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▅▁▅▁▁▅▅██▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_f1</td><td>▃▅▁▅▁▁▃▃██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_epoch</td><td>▆▅▄▄▅▅▅▅▁▁▃▄▇███████████████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_step</td><td>▆▅▄▄▅▅▅▅▁▁▃▄▇███████████████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.93333</td></tr><tr><td>test_f1</td><td>0.83333</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31355</td></tr><tr><td>train_loss_step</td><td>0.31355</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.70238</td></tr><tr><td>val_loss_step</td><td>0.70238</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/4dyhmjw1' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/4dyhmjw1</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194723-4dyhmjw1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194754-pj2vj49y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/pj2vj49y' target=\"_blank\">GINConv_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/pj2vj49y' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/pj2vj49y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\attention2\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_1\\attention2\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>██▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁▁████████</td></tr><tr><td>val_f1</td><td>▁▄██▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>val_loss_epoch</td><td>▃▃▁▁▅███████████████████████████████████</td></tr><tr><td>val_loss_step</td><td>▃▃▁▁▅███████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31332</td></tr><tr><td>train_loss_step</td><td>0.31332</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.2</td></tr><tr><td>val_auc</td><td>0.16667</td></tr><tr><td>val_f1</td><td>0.33333</td></tr><tr><td>val_loss_epoch</td><td>1.11208</td></tr><tr><td>val_loss_step</td><td>1.11208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/pj2vj49y' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/pj2vj49y</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194754-pj2vj49y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194822-f51j44kk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/f51j44kk' target=\"_blank\">GAT_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/f51j44kk' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/f51j44kk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\mean\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\mean\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.9000), 'test_f1': tensor(0.8000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▅▅▇▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▃▃▃▃▆▆▆▆▆▆▆▆▆▆▆▆███▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>▁▁▅▅███▅▅▅▅▅▅▅▅█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▃▃▃▃▅▅▅▅▅▅▅▅▅▅▅▅███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▇▆▅▅▄▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▇▇▇▆▅▅▄▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.9</td></tr><tr><td>test_f1</td><td>0.8</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31367</td></tr><tr><td>train_loss_step</td><td>0.31367</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.45325</td></tr><tr><td>val_loss_step</td><td>0.45325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/f51j44kk' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/f51j44kk</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194822-f51j44kk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194850-22n352d5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/22n352d5' target=\"_blank\">GINConv_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/22n352d5' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/22n352d5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\mean\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\mean\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▆█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▅▁▁▅██████████████████████████████████</td></tr><tr><td>val_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▅█▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_loss_step</td><td>▁▁▅█▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31332</td></tr><tr><td>train_loss_step</td><td>0.31332</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.7114</td></tr><tr><td>val_loss_step</td><td>0.7114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/22n352d5' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/22n352d5</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194850-22n352d5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194921-clzgi1bd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/clzgi1bd' target=\"_blank\">GAT_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/clzgi1bd' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/clzgi1bd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\max\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\max\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.3333), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▂▂▄▃▄▁▃▄▅▅▅▄▄▅▇▅▄▅▇▆▄▅▆▆▅▆▇▄▇▇▄▇▇▄▇▇█▆▆</td></tr><tr><td>train_auc</td><td>▂▂▂▇▂▃▃▁▆▅▅▄▃▅▆▇▅▅▆▇▆▆▇▅▅▅▇▇▅▇█▅▆█▄█▆██▇</td></tr><tr><td>train_f1</td><td>▂▃▄▅▄▅▁▄▅▆▆▆▅▅▆▇▆▅▆▇▆▅▆▆▆▆▆▇▅▇▇▅▇▇▅▇▇█▆▆</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▇▆█▇▅▅▅▄▆▅▄▃▄▅▄▃▃▅▅▃▃▄▃▂▅▂▂▅▃▂▅▁▃▁▃▃</td></tr><tr><td>train_loss_step</td><td>▄█▆▄▅▂▆▂▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▃▃▃▁▁▃▃▃██▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▂▇▆▆▇███████████▇▇██████▇▇▇▇███████████</td></tr><tr><td>val_loss_step</td><td>▁▂▇▆▆▇███████████▇▇██████▇▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.33333</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.83333</td></tr><tr><td>train_auc</td><td>0.9375</td></tr><tr><td>train_f1</td><td>0.85714</td></tr><tr><td>train_loss_epoch</td><td>0.46658</td></tr><tr><td>train_loss_step</td><td>0.46658</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.0</td></tr><tr><td>val_f1</td><td>0.57143</td></tr><tr><td>val_loss_epoch</td><td>0.91298</td></tr><tr><td>val_loss_step</td><td>0.91298</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/clzgi1bd' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/clzgi1bd</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194921-clzgi1bd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_194949-kfd013u1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kfd013u1' target=\"_blank\">GINConv_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kfd013u1' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kfd013u1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\max\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\max\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8667), 'test_f1': tensor(0.7273)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅██████████████████████████▇███████████</td></tr><tr><td>train_auc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆██████████████████████████▇███████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▁▁▁▁▅▁▅▅▅▅▅▅▁▁▁▅▅▅▁▁▁▁▅▁███████████▅▅</td></tr><tr><td>val_auc</td><td>▆▆▆▄▃▁▃▃▃▃▃▃▃▆▆▆▆▆▆▆▄▄▄▃▃▄▃▆▆███████████</td></tr><tr><td>val_f1</td><td>▄▄▆▄▄▄▄▆▁▆▆▆▆▆▆▄▄▄▆▆▆▁▁▄▄▆▁███████████▆▆</td></tr><tr><td>val_loss_epoch</td><td>▆▅▅▆▇▇▆▅▆▅▅▅▅▄▅▇▇▆▅▄▄▆▇██▅▆▂▂▂▂▁▁▁▁▁▁▁▃▄</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▇▇▆▅▆▅▅▅▅▄▅▇▇▆▅▄▄▆▇██▅▆▂▂▂▂▁▁▁▁▁▁▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.86667</td></tr><tr><td>test_f1</td><td>0.72727</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31366</td></tr><tr><td>train_loss_step</td><td>0.31366</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.68153</td></tr><tr><td>val_loss_step</td><td>0.68153</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kfd013u1' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kfd013u1</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_194949-kfd013u1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195017-e5yigg35</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/e5yigg35' target=\"_blank\">GAT_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/e5yigg35' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/e5yigg35</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\attention\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\attention\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.6000), 'test_f1': tensor(0.5000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▂▂▃▃▁▂▄▅▄▄▄▄▄▄▄▃▄▅▆▇▅▆▆▆▇█▆▆▅▇▆▇▆▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▃▃▄▂▂▆▅▃▆▄▆▆▆▄▄▄▄▇▇▅▇▅▇██▆▇▅▇▇▆▅▇▇█▇▇█</td></tr><tr><td>train_f1</td><td>▂▄▄▄▄▁▁▄▅▄▅▅▄▃▄▅▄▄▅▆▇▅▆▆▆▆█▆▆▅▇▆▇▆▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇█▇▆▆▇▅▆▄▆▅▅▇▇▅▄▃▅▃▄▄▃▁▃▄▅▃▃▃▄▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>██▃▇█▃▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▄▄▄▁██████▄██▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄██████████</td></tr><tr><td>val_auc</td><td>▃▂▁▁▅▆▆▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇█▇▇▆▆▆▇</td></tr><tr><td>val_f1</td><td>███▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▄▆▅▄▂▂▂▂▂▂▂▂▂▄▇▆▆▇▇▇▇▇██▇▇██▇▇▃▂▁▁▁▁▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▄▆▅▄▂▂▂▂▂▂▂▂▂▄▇▆▆▇▇▇▇▇██▇▇██▇▇▃▂▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.6</td></tr><tr><td>test_f1</td><td>0.5</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>0.94737</td></tr><tr><td>train_loss_epoch</td><td>0.37265</td></tr><tr><td>train_loss_step</td><td>0.37265</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67108</td></tr><tr><td>val_loss_step</td><td>0.67108</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/e5yigg35' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/e5yigg35</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195017-e5yigg35\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195046-2a0shzcw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/2a0shzcw' target=\"_blank\">GINConv_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/2a0shzcw' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/2a0shzcw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\attention\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\attention\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.9091), 'test_auc': tensor(0.8333), 'test_f1': tensor(0.9231)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅█████████████▇███▇███████▇████████▇█▇█</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▄▁▁█▁▁▁▇▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▆▆▆▆▃▃▃▃▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▇▅▇██████████▄▄███████▄▆▇█████▁▁▁▁██████</td></tr><tr><td>val_loss_step</td><td>▇▅▇██████████▄▄███████▄▆▇█████▁▁▁▁██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.90909</td></tr><tr><td>test_auc</td><td>0.83333</td></tr><tr><td>test_f1</td><td>0.92308</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31581</td></tr><tr><td>train_loss_step</td><td>0.31581</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.7129</td></tr><tr><td>val_loss_step</td><td>0.7129</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/2a0shzcw' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/2a0shzcw</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195046-2a0shzcw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195114-rvbf778t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rvbf778t' target=\"_blank\">GAT_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rvbf778t' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rvbf778t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\attention2\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_2\\attention2\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.6667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▁▁▅▅▆▇█████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▂▁▁▄▄▆▇█████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▅▅▅████▅▅▅▅▅▅████████████▅▅▅▅██▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▁█████████▅▅▅▅▅▅████████████████████████</td></tr><tr><td>val_f1</td><td>▆▆▆▇▇▇█▇▇▇▁▁▁▁▁▁▇▇▇▇▇▇▇▇▇▇▇▇▁▁▁▁▇▇▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▆▄▂▁▃▄▇▇▇▇▇▅▄▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▄▄▅▅▆▆▆▆</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▄▂▁▃▄▇▇▇▇▇▅▄▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▄▄▅▅▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.66667</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31343</td></tr><tr><td>train_loss_step</td><td>0.31343</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66203</td></tr><tr><td>val_loss_step</td><td>0.66203</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rvbf778t' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rvbf778t</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195114-rvbf778t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195142-89m41xch</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/89m41xch' target=\"_blank\">GINConv_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/89m41xch' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/89m41xch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\attention2\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_2\\attention2\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁██▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▆▆██▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▂▁▂▇████████████████████████████████████</td></tr><tr><td>val_loss_step</td><td>▂▁▂▇████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31328</td></tr><tr><td>train_loss_step</td><td>0.31328</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.33333</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.71326</td></tr><tr><td>val_loss_step</td><td>0.71326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/89m41xch' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/89m41xch</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195142-89m41xch\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195210-vmezhm53</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vmezhm53' target=\"_blank\">GAT_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vmezhm53' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vmezhm53</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\mean\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\mean\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▆▆▆▆▇▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▄████████▄█████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▄▄▄▁▁▁▁▁▁████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▆▇▇▆▆█▆▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▆▆▆▆▆▆▅▅▅</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▇▇▆▆█▆▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▆▆▆▆▆▆▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31385</td></tr><tr><td>train_loss_step</td><td>0.31385</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.62036</td></tr><tr><td>val_loss_step</td><td>0.62036</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vmezhm53' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vmezhm53</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195210-vmezhm53\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195237-sxg09myh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sxg09myh' target=\"_blank\">GINConv_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sxg09myh' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sxg09myh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\mean\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\mean\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇▇███████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅█▅███████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▄▄▄▄▄▄▄███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▆█▇███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▃▂▁▁▁▂▂▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▄▃▂▁▁▁▂▂▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.95062</td></tr><tr><td>train_f1</td><td>0.94737</td></tr><tr><td>train_loss_epoch</td><td>0.36888</td></tr><tr><td>train_loss_step</td><td>0.36888</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.51531</td></tr><tr><td>val_loss_step</td><td>0.51531</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sxg09myh' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sxg09myh</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195237-sxg09myh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195307-fg0dmg8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fg0dmg8k' target=\"_blank\">GAT_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fg0dmg8k' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fg0dmg8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\max\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\max\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.5667), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▄▅▁▃▂▄▄▅▅▄▆▄▄▄▄▃▇▅▇▇▇▆▆▇▇▇▆▇█▇▇▇▇▆▇▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▄▇▃▄▃▄▅▆▆▄▆▆▅▅▅▆▇▇█▇█▇▇███▇██▇▇███▇▇▇██</td></tr><tr><td>train_f1</td><td>▃▄▅▁▃▃▄▄▅▅▃▆▄▄▄▃▃▇▅▇▇▇▆▆▇▇▆▆▇█▇▇▇▇▆▆▆▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▇▇▇▆▆▄▄▆▃▄▆▆▅▆▃▄▂▃▃▃▃▂▂▂▃▂▁▃▃▂▃▃▃▃▃▂▂</td></tr><tr><td>train_loss_step</td><td>█▄▆▅▅▁▃▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>██▁▁█████▁███████████████████████▁██████</td></tr><tr><td>val_auc</td><td>▅█▆▅▁▁████████████████▆▆▆▆▆█████████████</td></tr><tr><td>val_f1</td><td>█▆▁▄█████▄▆██████████████████████▄▆▆▆▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>▆▇███▇▆▅▄▃▂▁▁▁▁▁▂▅▆▇▇▇▅▆▇▇▇▇▇▇▇▆▅▄▄▅▆▆▆▆</td></tr><tr><td>val_loss_step</td><td>▆▇███▇▆▅▄▃▂▁▁▁▁▁▂▅▆▇▇▇▅▆▇▇▇▇▇▇▇▆▅▄▄▅▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.56667</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.98765</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.37076</td></tr><tr><td>train_loss_step</td><td>0.37076</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.68839</td></tr><tr><td>val_loss_step</td><td>0.68839</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fg0dmg8k' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fg0dmg8k</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195307-fg0dmg8k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195334-bdkau39h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bdkau39h' target=\"_blank\">GINConv_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bdkau39h' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bdkau39h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\max\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\max\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.8000), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇█▇██████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▂▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▁▁▃▃▆▃▆▆▁▁▁▃▃▃▆███████████████████████</td></tr><tr><td>val_auc</td><td>▃▃▁▃▃▅█▆▆▅▃▁▅███████████████████████████</td></tr><tr><td>val_f1</td><td>▃▂▁▁▄▄▆▅▆▆▁▁▃▅▅▅▆███████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇▆▆▆▄▅▄▄▇█▇▅▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▆▆▇▆▆▆▄▅▄▄▇█▇▅▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.8</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31328</td></tr><tr><td>train_loss_step</td><td>0.31328</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>1.0</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.38012</td></tr><tr><td>val_loss_step</td><td>0.38012</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bdkau39h' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bdkau39h</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195334-bdkau39h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195403-rf31jpl5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rf31jpl5' target=\"_blank\">GAT_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rf31jpl5' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rf31jpl5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\attention\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\attention\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7000), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▅▄▄▅▆▄▇▅▇▆▇▅▇▆▅▇▇▅▇▆▇▇▇▇▇▇▇▇▇▇▆▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▄▆▅▅▆▃▅▅▇▆▇▇█▆▆█▆▆█▇▆▆▇▆██▇▇█▇▆▆█▇█▇</td></tr><tr><td>train_f1</td><td>▁▅▆▇▆▅▅▆▆▅▇▆▇▆▇▆▇▆▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▇▆▆▅▄▇▄▅▃▄▃▅▂▄▅▃▄▅▂▄▃▃▃▃▂▂▂▂▃▄▄▃▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▃▂▁▆▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆▃▆▆▆▆▆▆▆▆▆▆▆▃▃▃▃▃▃▃▃▆███▆▃▆▆▆███████▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▄▂▁▂▃▅▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_loss_step</td><td>▄▂▁▂▃▅▆▆▇▇▇▇▇▇██████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.7</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.93827</td></tr><tr><td>train_f1</td><td>0.94737</td></tr><tr><td>train_loss_epoch</td><td>0.36928</td></tr><tr><td>train_loss_step</td><td>0.36928</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.713</td></tr><tr><td>val_loss_step</td><td>0.713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rf31jpl5' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rf31jpl5</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195403-rf31jpl5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195433-sprg0n2i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sprg0n2i' target=\"_blank\">GINConv_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sprg0n2i' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sprg0n2i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\attention\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\attention\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.5000), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▇▇██████████████████████▇▇████████████</td></tr><tr><td>train_f1</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▃▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▆█▆▃▃▃▃▆▆▆███▆▆▆▃▃▆▆▆█████████████████</td></tr><tr><td>val_auc</td><td>█▁▁▁▁▁▁▁▁▅█████▅▅▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>█▁▆█▆▄▄▄▄▆▆▆███▅▅▅▄▄▆▆▆█████████████████</td></tr><tr><td>val_loss_epoch</td><td>▄▄▃▂▄▅██▇▄▄▄▁▁▂▅▅▆▆▇▅▄▃▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▄▄▃▂▄▅██▇▄▄▄▁▁▂▅▅▆▆▇▅▄▃▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.5</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.90123</td></tr><tr><td>train_f1</td><td>0.94737</td></tr><tr><td>train_loss_epoch</td><td>0.36886</td></tr><tr><td>train_loss_step</td><td>0.36886</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.55921</td></tr><tr><td>val_loss_step</td><td>0.55921</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sprg0n2i' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/sprg0n2i</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195433-sprg0n2i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195501-xuuawpcx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xuuawpcx' target=\"_blank\">GAT_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xuuawpcx' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xuuawpcx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\attention2\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_3\\attention2\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.6667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▅▅▆▅▇█████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▃▄▅▄▇█████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▅█████▅█████████████▅▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>██▅▁▁▁▅███▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>█▆████▇▃▇▇▇██████████▅▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_epoch</td><td>▇▆▅▄▄▃▃▆▂▁▂▂▂▂▂▂▂▂▂▃▄▆▇▇███████▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_step</td><td>▇▆▅▄▄▃▃▆▂▁▂▂▂▂▂▂▂▂▂▃▄▆▇▇███████▇▇▇▇▇▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.66667</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31348</td></tr><tr><td>train_loss_step</td><td>0.31348</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.6626</td></tr><tr><td>val_loss_step</td><td>0.6626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xuuawpcx' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xuuawpcx</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195501-xuuawpcx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195528-xy8i6l5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xy8i6l5r' target=\"_blank\">GINConv_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xy8i6l5r' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xy8i6l5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\attention2\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_3\\attention2\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.7500)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▃▁███████▆▆▆▆███████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆███████████████████████</td></tr><tr><td>val_f1</td><td>▁▁███████▆▆▆▆███████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▁▁▁▁▁▂▃▄▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▂▁▁▁▁▁▂▃▄▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.75</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.88889</td></tr><tr><td>train_f1</td><td>0.94737</td></tr><tr><td>train_loss_epoch</td><td>0.36885</td></tr><tr><td>train_loss_step</td><td>0.36885</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.52222</td></tr><tr><td>val_loss_step</td><td>0.52222</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xy8i6l5r' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/xy8i6l5r</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195528-xy8i6l5r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195555-54bal6d2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/54bal6d2' target=\"_blank\">GAT_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/54bal6d2' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/54bal6d2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\mean\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\mean\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.7143)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▅▅▇▇▇█████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▆▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▆▅▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▆▆▆▆█▆▆▆▆▆▆▆██████▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▆▆▆</td></tr><tr><td>val_auc</td><td>██████████████████████████████▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>val_f1</td><td>▁▄▇▇▇▇█▇▇▇▇▇▇▇██████▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▃</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.71429</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31472</td></tr><tr><td>train_loss_step</td><td>0.31472</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.48886</td></tr><tr><td>val_loss_step</td><td>0.48886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/54bal6d2' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/54bal6d2</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195555-54bal6d2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195623-8bwncwqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8bwncwqe' target=\"_blank\">GINConv_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8bwncwqe' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8bwncwqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\mean\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\mean\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.7143)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆▆███▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>▁▁██████████▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████████████</td></tr><tr><td>val_f1</td><td>▁▁▇▇▇▇███▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>▇█▅▄▃▂▁▁▁▃▅▆▆▆▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▄▄▃</td></tr><tr><td>val_loss_step</td><td>▇█▅▄▃▂▁▁▁▃▅▆▆▆▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.71429</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31332</td></tr><tr><td>train_loss_step</td><td>0.31332</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.4901</td></tr><tr><td>val_loss_step</td><td>0.4901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8bwncwqe' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8bwncwqe</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195623-8bwncwqe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195653-g52fgp01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/g52fgp01' target=\"_blank\">GAT_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/g52fgp01' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/g52fgp01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\max\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\max\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.2667), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▅▁▄▃▅▂▅▅▅▂▆▄▆▇▆▆▆▇▆▇█▇▇█▇▅▇▇▇█████▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▄▆▂▅▅▆▅▆▆▆▅▇▅▇██▇▆█▇▇█▇▇██▆██████████▇▇</td></tr><tr><td>train_f1</td><td>▂▄▅▁▅▃▄▁▅▅▅▂▆▃▅▆▆▅▆▇▆▆█▆▇█▇▅▇▇▇█████▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▇▅▅▅▆▄▄▅▆▃▅▃▂▄▃▃▂▄▃▁▃▃▁▂▄▂▂▂▁▁▁▁▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▄▄▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▆▆███▆▆▆▃▃▃▆▆▆▆▆▆▆▆▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▂▂▃▄▄▂▁▁▁▂▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▂▂▃▄▄▂▁▁▁▂▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.26667</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.90625</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.36887</td></tr><tr><td>train_loss_step</td><td>0.36887</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.51908</td></tr><tr><td>val_loss_step</td><td>0.51908</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/g52fgp01' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/g52fgp01</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195653-g52fgp01\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195722-hibcz47x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/hibcz47x' target=\"_blank\">GINConv_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/hibcz47x' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/hibcz47x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\max\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\max\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.6667)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆███████████████████████████▇████████</td></tr><tr><td>train_auc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▆▆███████████████████████████▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▅██▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>▃▁▅▅████████▆▆▆▆▆█████▆▆▆███████████████</td></tr><tr><td>val_f1</td><td>▁▁▆▆██▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▄▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▃▃▂▂▁▁▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▃▃▂▂▁▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.66667</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31327</td></tr><tr><td>train_loss_step</td><td>0.31327</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.56356</td></tr><tr><td>val_loss_step</td><td>0.56356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/hibcz47x' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/hibcz47x</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195722-hibcz47x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195750-u6x8q1me</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/u6x8q1me' target=\"_blank\">GAT_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/u6x8q1me' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/u6x8q1me</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\attention\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\attention\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▁▂▃▄▆▆▆▄▆▅▆▆▇▇▅█▆▇▆▇▆▇▆▆▇▇██▇▆▆█▇▆▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▃▂▅▆▆▇▆▆▇▆▇▇██▆█▇█▇▇▆█▆█▇███████▇█▇████</td></tr><tr><td>train_f1</td><td>▃▁▁▄▆▇▇▇▆▇▆▇▇██▆█▇█▇█▇█▇▇█████▇▇██▇█████</td></tr><tr><td>train_loss_epoch</td><td>███▆▆▄▄▅▅▃▅▃▃▂▂▅▁▃▂▃▂▄▂▃▂▂▂▁▁▂▂▃▁▂▃▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▅▇▃▄▁▄▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁█████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁█████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.9625</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.3692</td></tr><tr><td>train_loss_step</td><td>0.3692</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.51353</td></tr><tr><td>val_loss_step</td><td>0.51353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/u6x8q1me' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/u6x8q1me</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195750-u6x8q1me\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195817-x0o9tsdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/x0o9tsdi' target=\"_blank\">GINConv_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/x0o9tsdi' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/x0o9tsdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\attention\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\attention\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.035     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.7667), 'test_f1': tensor(0.7273)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇██████████████▇▇████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▂▆▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆▅▅▅▃▅███▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>█▅▅▅██▅▅▁████████▅▅▁▅▅▅▅▅▅██████████████</td></tr><tr><td>val_f1</td><td>▁▁▇▇▇▆▆▆▄▆███▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▄▃▄▅▆▆█▅▂▁▁▄▆▆▆▆▆▆▆▆▆▆▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>▆▆▄▃▄▅▆▆█▅▂▁▁▄▆▆▆▆▆▆▆▆▆▆▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.76667</td></tr><tr><td>test_f1</td><td>0.72727</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.3133</td></tr><tr><td>train_loss_step</td><td>0.3133</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.51405</td></tr><tr><td>val_loss_step</td><td>0.51405</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/x0o9tsdi' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/x0o9tsdi</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195817-x0o9tsdi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195847-72x0ivvo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/72x0ivvo' target=\"_blank\">GAT_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/72x0ivvo' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/72x0ivvo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\attention2\\GraphLevelGAT\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GAT_3_32_4\\attention2\\GraphLevelGAT exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GAT              | 24.0 K | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7000), 'test_f1': tensor(0.6154)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▇█▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▆▆▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆███████████▆▆▆▆▆███████████████████</td></tr><tr><td>val_auc</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▇▇▇███████████▇▇▇▇▇███████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▆▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>██▆▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.7</td></tr><tr><td>test_f1</td><td>0.61538</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31347</td></tr><tr><td>train_loss_step</td><td>0.31347</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>1.0</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.37424</td></tr><tr><td>val_loss_step</td><td>0.37424</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/72x0ivvo' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/72x0ivvo</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195847-72x0ivvo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195914-8rzs9fa0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8rzs9fa0' target=\"_blank\">GINConv_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8rzs9fa0' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8rzs9fa0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\attention2\\GraphLevelGINConv\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\GINConv_3_32_4\\attention2\\GraphLevelGINConv exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | GIN              | 7.5 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8667), 'test_f1': tensor(0.7273)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁████▆█▆▆▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆██████████</td></tr><tr><td>val_auc</td><td>▃▁███▆▆▆▅▃▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_f1</td><td>▁▁████▇█▇▇▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val_loss_epoch</td><td>█▅▂▁▁▂▃▂▄▄▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▂▁▁▂▃▂▄▄▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.86667</td></tr><tr><td>test_f1</td><td>0.72727</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31335</td></tr><tr><td>train_loss_step</td><td>0.31335</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.53915</td></tr><tr><td>val_loss_step</td><td>0.53915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8rzs9fa0' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8rzs9fa0</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195914-8rzs9fa0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Print K fold model number of samples and number of positive cases\n",
    "\n",
    "k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "kfold = StratifiedShuffleSplit(n_splits=k_folds, test_size=0.3, random_state=0)\n",
    "val_size = 0.2\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, ys)):\n",
    " # train_subset = dataset.index_select(train_ids.tolist())\n",
    " test_subset = dataset.index_select(test_ids.tolist())\n",
    "\n",
    " train_ids, valid_ids = train_test_split(train_ids, test_size = val_size, random_state=0)\n",
    " train_subset = dataset.index_select(train_ids.tolist())\n",
    " val_subset = dataset.index_select(valid_ids.tolist())\n",
    "\n",
    " for pool in pools:\n",
    "     for model in models:\n",
    "         if model == 'GAT':\n",
    "             batch_size=64\n",
    "         else:\n",
    "             batch_size=128\n",
    "         # Path to the folder where the pretrained models are saved\n",
    "         CHECKPOINT_PATH = checkpoint_folder / f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}' / pool\n",
    "         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "         # Skip already trained kfold and pool\n",
    "         checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "         if checkpoint.exists():\n",
    "             print(checkpoint)\n",
    "             continue\n",
    "\n",
    "         # Run training\n",
    "         run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}', \n",
    "                         group=f'{model}_{pool}')\n",
    "         graph.train_graph_classifier_kfold(model, \n",
    "                                              train_subset, \n",
    "                                              val_subset, \n",
    "                                              test_subset,\n",
    "                                              dataset, \n",
    "                                              CHECKPOINT_PATH, \n",
    "                                              AVAIL_GPUS, \n",
    "                                              hidden_channels=HIDDEN_CHANNELS, \n",
    "                                              num_layers=NUM_LAYERS, \n",
    "                                              epochs=epochs,\n",
    "                                              batch_size=batch_size,\n",
    "                                              graph_pooling=pool,\n",
    "                                              morph=False,\n",
    "                                              pos=True)\n",
    "         run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [0]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 3\n",
    "HIDDEN_CHANNELS = 32\n",
    "pools = ['mean', 'max', 'attention', 'attention2']\n",
    "models = ['MLP']\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [data.y.item() for data in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_195942-vjkpezrp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vjkpezrp' target=\"_blank\">MLP_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vjkpezrp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vjkpezrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\mean\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.8333), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▇▇▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▆▅▅▇▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▄▁▁▄▅▆▆▆████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▁█▅▅▅▅▅▁▅███▅████▅▅████████████████████</td></tr><tr><td>val_auc</td><td>▅▅▅▅▁▁▁▁▁▁▅▅██▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅███</td></tr><tr><td>val_f1</td><td>▆▁█▆▆▆▆▆▄▆███▅████▆▆████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▅█▆▅▅▆▆▆▅▃▁▁▁▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▅█▆▅▅▆▆▆▅▃▁▁▁▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.83333</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31382</td></tr><tr><td>train_loss_step</td><td>0.31382</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.49252</td></tr><tr><td>val_loss_step</td><td>0.49252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vjkpezrp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/vjkpezrp</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_195942-vjkpezrp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200014-rrp79p42</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rrp79p42' target=\"_blank\">MLP_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rrp79p42' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rrp79p42</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\max\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.4545), 'test_auc': tensor(0.4667), 'test_f1': tensor(0.)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▂▁▆█▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>train_auc</td><td>▄▁▂▄▄▅▂▄▃▄▃▄▇▂▇▃▃▄▇▅▅▅▃▄▇▆▇▄▄▆▅▇▅▄▆▁█▄▇▄</td></tr><tr><td>train_f1</td><td>▇▇▇█▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▆▇█▃▁▇▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>████▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>████▆▆▆▆▆▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>████▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▁▃▆██████████████████████████████████</td></tr><tr><td>val_loss_step</td><td>▂▂▂▁▃▆██████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.45455</td></tr><tr><td>test_auc</td><td>0.46667</td></tr><tr><td>test_f1</td><td>0.0</td></tr><tr><td>train_acc</td><td>0.55556</td></tr><tr><td>train_auc</td><td>0.55</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.75771</td></tr><tr><td>train_loss_step</td><td>0.75771</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.2</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>1.11326</td></tr><tr><td>val_loss_step</td><td>1.11326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rrp79p42' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rrp79p42</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200014-rrp79p42\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200043-tgps0xgs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tgps0xgs' target=\"_blank\">MLP_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tgps0xgs' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tgps0xgs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\attention\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.4545), 'test_auc': tensor(0.7000), 'test_f1': tensor(0.)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▂▁▄▄▃▃▆▅▄▃▃▃▄▅▆▃▆▆▆▃▅▆▆▆▆▆▅▆▅▆▄▆█▅▆▅▆▆▅</td></tr><tr><td>train_auc</td><td>▆▂▁▅▄▃▅▆▆▅▃▅▄▅▆▇▄▆▇█▃▅▇▅█▆▇▅▆▆▆▆▇▇▇█▅▆█▆</td></tr><tr><td>train_f1</td><td>▅▅▁▅▄▃▄▆▆▄▃▅▅▆▅▇▄▇▇▇▃▅▇▆▆▇▇▅▆▆▆▅▇█▄▇▆▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▄▄▅▄▄▄▅▇▅▆▄▃▂▆▂▂▂▅▄▂▃▂▂▂▃▃▄▂▄▂▁▃▁▄▃▁▃</td></tr><tr><td>train_loss_step</td><td>█▄▄▁▄▂▄▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅███▅▅▁▁▁▅▅▅▅███████████▅▅▅▅▅▁▁▁▅▅█▅███▅</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▄▆▆▆▆▅▅▆▇▇▇▇▇████▇▇▇▇▆▆▅▄▅▆▇▇▆▆▇███▇▆▇█</td></tr><tr><td>val_loss_step</td><td>▁▄▆▆▆▆▅▅▆▇▇▇▇▇████▇▇▇▇▆▆▅▄▅▆▇▇▆▆▇███▇▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.45455</td></tr><tr><td>test_auc</td><td>0.7</td></tr><tr><td>test_f1</td><td>0.0</td></tr><tr><td>train_acc</td><td>0.72222</td></tr><tr><td>train_auc</td><td>0.8125</td></tr><tr><td>train_f1</td><td>0.66667</td></tr><tr><td>train_loss_epoch</td><td>0.53247</td></tr><tr><td>train_loss_step</td><td>0.53247</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.2</td></tr><tr><td>val_auc</td><td>0.75</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>1.06059</td></tr><tr><td>val_loss_step</td><td>1.06059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tgps0xgs' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/tgps0xgs</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200043-tgps0xgs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200112-7f44i28o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/7f44i28o' target=\"_blank\">MLP_3_32_0</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/7f44i28o' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/7f44i28o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\attention2\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.8333), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▅▅▅▆▆▇▆█▇▇████████▇█████████████████</td></tr><tr><td>train_auc</td><td>▁▃▆▆▆▆▇█████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▃▅▆▆▆▇▇█▇█████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▅▄▃▂▂▃▁▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▅▁▁▅▅▅█▁█▁▅██▅█████▅██▅██▅█▅████▅████</td></tr><tr><td>val_auc</td><td>▅▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██</td></tr><tr><td>val_f1</td><td>█▁▁▅▁▃▆▆▆█▃█▃▆██▆█████▅██▆██▆█▆████▆████</td></tr><tr><td>val_loss_epoch</td><td>▅▆▆▇█▇▆▅▄▂▆▂▆▄▃▃▃▂▂▂▂▂▆▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▅▆▆▇█▇▆▅▄▂▆▂▆▄▃▃▃▂▂▂▂▂▆▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.83333</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.32425</td></tr><tr><td>train_loss_step</td><td>0.32425</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.88889</td></tr><tr><td>val_loss_epoch</td><td>0.50472</td></tr><tr><td>val_loss_step</td><td>0.50472</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_0</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/7f44i28o' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/7f44i28o</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200112-7f44i28o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200140-r92c8gyn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/r92c8gyn' target=\"_blank\">MLP_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/r92c8gyn' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/r92c8gyn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\mean\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.8182), 'test_auc': tensor(0.8667), 'test_f1': tensor(0.8571)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▆▆█▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▄▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▆▇▆▆█▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▁▁▁▁▁▅▅▅▅▅▅█▅▅▅▅▅██████████████████████</td></tr><tr><td>val_f1</td><td>▄▄▄▄▁▁▁▁█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>val_loss_epoch</td><td>▂▂▄▅▆▆▄▃▁▁▄▄▅▇███▇▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▄▄▄▄▅▅▅</td></tr><tr><td>val_loss_step</td><td>▂▂▄▅▆▆▄▃▁▁▄▄▅▇███▇▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▄▄▄▄▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.81818</td></tr><tr><td>test_auc</td><td>0.86667</td></tr><tr><td>test_f1</td><td>0.85714</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31367</td></tr><tr><td>train_loss_step</td><td>0.31367</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.66864</td></tr><tr><td>val_loss_step</td><td>0.66864</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/r92c8gyn' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/r92c8gyn</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200140-r92c8gyn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200211-3k89dltz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/3k89dltz' target=\"_blank\">MLP_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/3k89dltz' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/3k89dltz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\max\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.4167), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▁▂▂▂▄▅▁▃▅▃▅▅▆▆▄▅▄▆▂█▄▄█▆▅▅▇▅▄▆█▅▆▇▅█</td></tr><tr><td>train_auc</td><td>▁▂▄▃▁▂▂▂▃▅▁▄▇▆▇▆▆▅▆▅▃▄▄█▆▅▇▆▃▅█▅▅▆█▇▇▆▆█</td></tr><tr><td>train_f1</td><td>▄▄▄▂▁▄▃▄▄▄▂▄▆▄▅▆▆▇▅▆▄▇▄█▁▃█▇▆▅▇▆▄▆█▆▇▇▆█</td></tr><tr><td>train_loss_epoch</td><td>███▆▇▇▇▇▆▅▇▆▅▆▄▄▃▃▅▄▆▃▇▁▅▅▁▂▄▅▂▄▅▃▁▄▃▂▄▁</td></tr><tr><td>train_loss_step</td><td>▃█▃▃▃▄▃▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>██▁▁▁██████████████████▁▁█████▁▁████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▂█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▂▁▁▁▁▃▄▁▁▁▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>▁▁▂█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▂▁▁▁▁▃▄▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.41667</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.98765</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.36587</td></tr><tr><td>train_loss_step</td><td>0.36587</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.25</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.71625</td></tr><tr><td>val_loss_step</td><td>0.71625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/3k89dltz' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/3k89dltz</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200211-3k89dltz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200240-bl9ca0in</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl9ca0in' target=\"_blank\">MLP_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl9ca0in' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl9ca0in</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\attention\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.6000), 'test_f1': tensor(0.2857)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▁▂▂▄▂▄▂▄▅▄▆▆▄▄▄▅▄▆▄▇▅▇▅▇▄▇▇▇▄▄▇█▃▇▆▇▇▇▆</td></tr><tr><td>train_auc</td><td>▅▁▃▃▃▅▇▄▄▆▅▇▆▆▆▅▆▆▇▇█▆█▆█▅▇█▇▇▇██▇█▇████</td></tr><tr><td>train_f1</td><td>▄▂▂▁▅▄▅▃▄▅▅▆▆▅▄▅▆▃▆▅▆▆▇▅▇▃▇▇▇▄▅▇█▁▆▆▇▆▇▆</td></tr><tr><td>train_loss_epoch</td><td>▇█▆▇▆▆▄▆▆▄▅▄▄▄▄▅▄▄▃▄▃▄▂▄▂▆▂▂▂▄▅▂▁▅▂▃▁▂▂▃</td></tr><tr><td>train_loss_step</td><td>█▄▂▆▁▂▂▆▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▃▄▆▆▆▁▁▃▃▃▃▃▃▄▃▃▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▆▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▄▄▄▄▄▅▆▆▇▆▇▇██▇██████████▇▇▇▆▃▃▇█▆▅▇▇▅▆</td></tr><tr><td>val_loss_step</td><td>▁▄▄▄▄▄▅▆▆▇▆▇▇██▇██████████▇▇▇▆▃▃▇█▆▅▇▇▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.6</td></tr><tr><td>test_f1</td><td>0.28571</td></tr><tr><td>train_acc</td><td>0.83333</td></tr><tr><td>train_auc</td><td>0.96296</td></tr><tr><td>train_f1</td><td>0.85714</td></tr><tr><td>train_loss_epoch</td><td>0.47667</td></tr><tr><td>train_loss_step</td><td>0.47667</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.83481</td></tr><tr><td>val_loss_step</td><td>0.83481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl9ca0in' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl9ca0in</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200240-bl9ca0in\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200308-8u8qufdp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8u8qufdp' target=\"_blank\">MLP_3_32_1</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8u8qufdp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8u8qufdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\attention2\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.6000), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▃▆▇▆▇▇▇██▇▇███████▇█▇█████████████████</td></tr><tr><td>train_auc</td><td>▁▆▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▅▇█▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▃▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▁▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▅██▅█▅▅▁▁▁▁▁▁▁▁▁▁▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_auc</td><td>██████▃▃▃▃▃▁▁▁▁▁▁▁▁▃▃▃▁▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▅▅██▃█▃▃▁▁▁▁▁▁▁▁▁▁▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▂▁▃▄▆▅▇██▇████▄█▆▃▃▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▄</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▂▁▃▄▆▅▇██▇████▄█▆▃▃▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.6</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31379</td></tr><tr><td>train_loss_step</td><td>0.31379</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.41667</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.72264</td></tr><tr><td>val_loss_step</td><td>0.72264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_1</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8u8qufdp' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/8u8qufdp</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200308-8u8qufdp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200337-p4x4e0bu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/p4x4e0bu' target=\"_blank\">MLP_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/p4x4e0bu' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/p4x4e0bu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\mean\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8000), 'test_f1': tensor(0.8000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▆▆▇████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▆█▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▄▅▆▆▆▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▆▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅█████████████████████████████</td></tr><tr><td>val_auc</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▄▄▄▄▄▄▄▄▄▄█████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▆▅▅▅▅▆▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▆▅▅▅▅▆▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.8</td></tr><tr><td>test_f1</td><td>0.8</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31375</td></tr><tr><td>train_loss_step</td><td>0.31375</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.53117</td></tr><tr><td>val_loss_step</td><td>0.53117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/p4x4e0bu' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/p4x4e0bu</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200337-p4x4e0bu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200406-bl2vrebw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl2vrebw' target=\"_blank\">MLP_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl2vrebw' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl2vrebw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\max\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.8667), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▅▃▃▃▅▆▅▅▅▆▅▅▂▅▅▆▆▁█▇▅▅▅▅▃▅▅▆▅▆▅▅▇▆█▃▆▆▅▇</td></tr><tr><td>train_auc</td><td>▄▅▃▃▆▄▅▄▆▄▅▄▁▅▄▅▆▁█▄▆▆▇▄▄▆▆▆▆█▅▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train_f1</td><td>▆▂▅▅▆▇▆▆▆▇▆▆▄▆▆▇▇▁▇█▆▆▆▆▅▆▆▇▆▇▆▃█▇█▅▇▇▆█</td></tr><tr><td>train_loss_epoch</td><td>▅▄▅▅▅▄▅▅▅▄▅▅▆▄▅▄▃█▁▃▅▅▅▅▅▅▅▄▄▄▄▃▃▄▁▅▄▄▅▃</td></tr><tr><td>train_loss_step</td><td>▇▇▁█▇▅▇▂▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁▁▁▁▁▁▁████████▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▄▄▂▄▄▄▅▅▅▅▄▂▂▄██████▅▅▅</td></tr><tr><td>val_f1</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁▁▁▁▁▁▁████████▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▄███████████▇▇▅▄▄▄█████████▇▄▄▃▄▂▁▄▄▇██</td></tr><tr><td>val_loss_step</td><td>█▄███████████▇▇▅▄▄▄█████████▇▄▄▃▄▂▁▄▄▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.86667</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.66667</td></tr><tr><td>train_auc</td><td>0.6875</td></tr><tr><td>train_f1</td><td>0.76923</td></tr><tr><td>train_loss_epoch</td><td>0.64775</td></tr><tr><td>train_loss_step</td><td>0.64775</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.57143</td></tr><tr><td>val_loss_epoch</td><td>0.91017</td></tr><tr><td>val_loss_step</td><td>0.91017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl2vrebw' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bl2vrebw</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200406-bl2vrebw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200433-gmowdqdj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/gmowdqdj' target=\"_blank\">MLP_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/gmowdqdj' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/gmowdqdj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\attention\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.6000), 'test_f1': tensor(0.4444)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▂▂▃▁▄▂▅▃▅▅▃▇▅▅▅▆▃▆▃▆▄▅▆▅▆▆▆▅▇▆▅▆█▅▅▅▅▆▆▅</td></tr><tr><td>train_auc</td><td>▃▁▄▃▄▁▄▄▅▅▃▇▇▆▇▆▄▆▅▇▅▅▆▇▇▇▇▇▆▆▆▇█▅▆▇█▆██</td></tr><tr><td>train_f1</td><td>▅▄▅▁▄▂▆▅▆▆▄▇▆▆▆▇▄▇▃▇▄▅▇▆▆▇▇▆▇▇▆▆█▆▆▆▅▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>██▆▇▆█▆▆▅▅▆▄▄▄▃▃▅▃▅▃▅▄▃▃▂▂▂▂▂▂▂▂▁▄▄▂▂▃▂▄</td></tr><tr><td>train_loss_step</td><td>█▆▆▆▆▄▂▄▁▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██</td></tr><tr><td>val_auc</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁██</td></tr><tr><td>val_f1</td><td>▇█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇▅▆███▆▅███▁▂</td></tr><tr><td>val_loss_step</td><td>▇▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇▅▆███▆▅███▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.6</td></tr><tr><td>test_f1</td><td>0.44444</td></tr><tr><td>train_acc</td><td>0.72222</td></tr><tr><td>train_auc</td><td>0.975</td></tr><tr><td>train_f1</td><td>0.8</td></tr><tr><td>train_loss_epoch</td><td>0.55927</td></tr><tr><td>train_loss_step</td><td>0.55927</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.55618</td></tr><tr><td>val_loss_step</td><td>0.55618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/gmowdqdj' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/gmowdqdj</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200433-gmowdqdj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200500-kunwrhxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kunwrhxc' target=\"_blank\">MLP_3_32_2</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kunwrhxc' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kunwrhxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\attention2\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.6154)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▂▄▅▆▇█▇▇█▇████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▇▄▇▇▆██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▆▆▇▇▇█████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▅▄▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▅▅▅█▁█▅█▁▅▅▁▅▅▅▅▁▅▅▅▅▅▅▅▅█▅▅▅█▅▅▅█▅▅▅</td></tr><tr><td>val_auc</td><td>▁▄██▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆██████████████████</td></tr><tr><td>val_f1</td><td>▄▄▄▃▃▃▆▁▆▃▆▁▃▃▁▃▃▆▆▁▃▃▃▆▆▆▆▆█▃▃▃█▆▆▆█▃▃▆</td></tr><tr><td>val_loss_epoch</td><td>▇█▆▃▂▂▁▄▁▃▂▅▄▆▆▄▄▅▅▅▄▄▄▃▃▃▃▂▂▃▃▃▂▃▄▄▂▃▄▂</td></tr><tr><td>val_loss_step</td><td>▇█▆▃▂▂▁▄▁▃▂▅▄▆▆▄▄▅▅▅▄▄▄▃▃▃▃▂▂▃▃▃▂▃▄▄▂▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.61538</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31396</td></tr><tr><td>train_loss_step</td><td>0.31396</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.83333</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.60636</td></tr><tr><td>val_loss_step</td><td>0.60636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_2</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kunwrhxc' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/kunwrhxc</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200500-kunwrhxc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200526-rmevlni0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rmevlni0' target=\"_blank\">MLP_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rmevlni0' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rmevlni0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\mean\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.7273), 'test_auc': tensor(0.8333), 'test_f1': tensor(0.7692)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▇▇███████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▃▂▄▅▆▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▅▅▄▆▅▇▇███████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▁▅▅▁▁▅▅████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▆▁▅▅▁▁▅▅████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▅▆▆▆██▇▆▃▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▅▆▆▆██▇▆▃▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.72727</td></tr><tr><td>test_auc</td><td>0.83333</td></tr><tr><td>test_f1</td><td>0.76923</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31388</td></tr><tr><td>train_loss_step</td><td>0.31388</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.57097</td></tr><tr><td>val_loss_step</td><td>0.57097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rmevlni0' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rmevlni0</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200526-rmevlni0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200554-cy2f4fst</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/cy2f4fst' target=\"_blank\">MLP_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/cy2f4fst' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/cy2f4fst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\max\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.4545), 'test_auc': tensor(0.4667), 'test_f1': tensor(0.5000)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▃▃▄▂▁▄▁▄▄▃▆▆▃▃▆▆▅▇▃▅▅▆▆▄▂▃▃▃▃▃▃▄▄▇▆▆▅▅█</td></tr><tr><td>train_auc</td><td>▁▃▄▄▁▁▃▂▅▄▆▆▆▄▃▄▆▆▅▅▇▄▆▅▅▆▆█▆▇▇▅▆█▇▆▇▅▆█</td></tr><tr><td>train_f1</td><td>▆▆▄▇▅▅▆▄▅▆▆▇▇▆▅▇▆▆▇▆▇▇▇▇▅▁▁▁▁▁▁▁▄▄▇▇▇▆▆█</td></tr><tr><td>train_loss_epoch</td><td>▆▆▆▅▇█▆▇▅▅▆▃▄▆▅▄▃▄▃▆▄▄▃▃▅▇▆▆▆▆▆▆▅▄▃▃▃▃▄▁</td></tr><tr><td>train_loss_step</td><td>▆▅▆▅▅█▆▅▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅██▅▅▅▅▅▅███▅▅▅▅██▁▁▁▁▁▁▁▁▅▅████▅▅</td></tr><tr><td>val_auc</td><td>▃▆▃▃▁▁▁▁▁▃▃▃▆██████████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▃</td></tr><tr><td>val_f1</td><td>▇▇▇▇▇▇▇██▇▇▇▇▇▇███▇▇▇▇██▁▁▁▁▁▁▁▁▅▆████▆▆</td></tr><tr><td>val_loss_epoch</td><td>▅▅▄▄▄▄▄▃▃▄▅▅▅▄▄▂▁▂▃▅▅▄▂▁▆███████▃▄▁▂▁▁▄▅</td></tr><tr><td>val_loss_step</td><td>▅▅▄▄▄▄▄▃▃▄▅▅▅▄▄▂▁▂▃▅▅▄▂▁▆███████▃▄▁▂▁▁▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.45455</td></tr><tr><td>test_auc</td><td>0.46667</td></tr><tr><td>test_f1</td><td>0.5</td></tr><tr><td>train_acc</td><td>0.88889</td></tr><tr><td>train_auc</td><td>0.96296</td></tr><tr><td>train_f1</td><td>0.9</td></tr><tr><td>train_loss_epoch</td><td>0.42432</td></tr><tr><td>train_loss_step</td><td>0.42432</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.70902</td></tr><tr><td>val_loss_step</td><td>0.70902</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/cy2f4fst' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/cy2f4fst</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200554-cy2f4fst\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200625-qazpglf7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qazpglf7' target=\"_blank\">MLP_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qazpglf7' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qazpglf7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\attention\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.7000), 'test_f1': tensor(0.5455)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▁▃▅▄▅▁▃▃▅▄▄▄▄▇▅▇▆▄▄▅▇▅▆▆▇▅▇▅▇▅▇▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▃▆▅▆▃▅▇▇▅▄▅▆▇▆▇▅▅▆▇█▇▇▇▇▆▇▆▇▆▇█▇█████</td></tr><tr><td>train_f1</td><td>▄▄▁▃▆▅▆▃▂▃▅▅▅▆▅▇▆▇▇▆▅▆▇▆▆▆▇▆▇▆▇▆▇▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▅▆▅▅▅▅▆▅▄▄▄▃▅▅▄▃▂▃▃▃▂▄▂▄▃▄▂▂▃▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▆▃▃▅▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▅▅▅▁▅▅▅▁▁▅▅▅▅▁▁▁▁▁▅▁▅▁▁▅▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>███▆▁▁▁▁▁▁▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▅▅▅▁▅▅▅▄▄▅▅▅▅▄▄▄▄▄▅▄▆▄▄▆▄▄▄▄▄</td></tr><tr><td>val_loss_epoch</td><td>▁▃▅▅▅▅▆▇▇▇▆▆▆▆▇▆▆▆▆▆▆▇▇▇▇▇██▇▇█▆██▆▇▇▆▇▇</td></tr><tr><td>val_loss_step</td><td>▁▃▅▅▅▅▆▇▇▇▆▆▆▆▇▆▆▆▆▆▆▇▇▇▇▇██▇▇█▆██▆▇▇▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.7</td></tr><tr><td>test_f1</td><td>0.54545</td></tr><tr><td>train_acc</td><td>0.94444</td></tr><tr><td>train_auc</td><td>0.98765</td></tr><tr><td>train_f1</td><td>0.94118</td></tr><tr><td>train_loss_epoch</td><td>0.41715</td></tr><tr><td>train_loss_step</td><td>0.41715</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.66667</td></tr><tr><td>val_f1</td><td>0.4</td></tr><tr><td>val_loss_epoch</td><td>0.7865</td></tr><tr><td>val_loss_step</td><td>0.7865</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qazpglf7' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qazpglf7</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200625-qazpglf7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200653-fbqhrfxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fbqhrfxr' target=\"_blank\">MLP_3_32_3</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fbqhrfxr' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fbqhrfxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\attention2\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.9091), 'test_auc': tensor(0.9667), 'test_f1': tensor(0.9091)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▄▄▆▇▄▆▄▄▆▇▆▆█▇▇▇▇██████▆███▇████████</td></tr><tr><td>train_auc</td><td>▁▃▅▆▆▆▇█▆▇▆█████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▆▅▆▆▇█▆▇▆▆▇█▇▇███████████▇████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▆▆▄▃▅▄▅▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▂▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>██▁██▁▁▁▁█▁██▁█▁▁▁▁▁▁█▁▁█████▁█▁████▁▁█▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>██▅▆▆▁▁▁▁▆▅▆▆▅▆▅▅▅▅▅▅▇▅▅▇▇▆▆▇▅▆▅▇▇▆▆▅▅▇▅</td></tr><tr><td>val_loss_epoch</td><td>▁▁▄▆▅▇▇█▇▅▆▃▄▇▄▅▅▄▄▄▄▃▅▅▄▃▃▅▄▇▃▇▃▃▃▃▅▇▃▅</td></tr><tr><td>val_loss_step</td><td>▁▁▄▆▅▇▇█▇▅▆▃▄▇▄▅▅▄▄▄▄▃▅▅▄▃▃▅▄▇▃▇▃▃▃▃▅▇▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.90909</td></tr><tr><td>test_auc</td><td>0.96667</td></tr><tr><td>test_f1</td><td>0.90909</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31363</td></tr><tr><td>train_loss_step</td><td>0.31363</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.4</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.4</td></tr><tr><td>val_loss_epoch</td><td>0.77766</td></tr><tr><td>val_loss_step</td><td>0.77766</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_3</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fbqhrfxr' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/fbqhrfxr</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200653-fbqhrfxr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200720-qqqnp8xe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qqqnp8xe' target=\"_blank\">MLP_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qqqnp8xe' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qqqnp8xe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\mean\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.6667), 'test_f1': tensor(0.6154)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▅▇▇█▇▇█████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▅▅▅▆▇█████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▅▅▅▇▇█▇▇█████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▆▁▆▆▆▆▆███▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_auc</td><td>▁███████████▁▁▁▁▁▁▁▁▁▁██████████████████</td></tr><tr><td>val_f1</td><td>▇▁▇▇▇▇▇███▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅█▅▄▄▄▃▂▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>▅█▅▄▄▄▃▂▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.66667</td></tr><tr><td>test_f1</td><td>0.61538</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.31368</td></tr><tr><td>train_loss_step</td><td>0.31368</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>0.85714</td></tr><tr><td>val_loss_epoch</td><td>0.49741</td></tr><tr><td>val_loss_step</td><td>0.49741</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qqqnp8xe' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/qqqnp8xe</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200720-qqqnp8xe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200747-bwo4ngns</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bwo4ngns' target=\"_blank\">MLP_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bwo4ngns' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bwo4ngns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\max\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.5455), 'test_auc': tensor(0.4000), 'test_f1': tensor(0.7059)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▁▁▅▅▃▃▄▂▆▄▄▃▄▃▆▃▆▆▅▆▆▄▅▇▇▄▆▆▄▄█▆▄▃█▇</td></tr><tr><td>train_auc</td><td>▅▃▂▂▁▃▅▄▄▃▃▃▄▂▅▄▆▆▆▄▇▇▆▇▆▆▇▇█▅▇▆█▆█▆▇▃█▇</td></tr><tr><td>train_f1</td><td>▄▄▂▃▃▃▄▄▂▁▄▃▆▄▄▂▃▅▆▃▅▆▅▆▆▂▄▇▇▃▆▆▆▅█▅▃▄█▇</td></tr><tr><td>train_loss_epoch</td><td>██▅▆▇█▄▄▅▆▅▆▄▅▄▅▄▅▃▆▃▂▄▃▃▄▄▂▂▅▃▃▄▄▁▃▅▅▁▂</td></tr><tr><td>train_loss_step</td><td>▄█▃▄▂▅▄▅▇▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▆▅▅▆▆█▅▁▁▁██████████▃█▆▆▃▁▃▃▅▆████▆▁▁▆██</td></tr><tr><td>val_auc</td><td>█▁▁▁████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▆▆▇▇█▆▁▁▁██████████▄█▇▇▄▁▄▄▆▇████▇▁▁▇██</td></tr><tr><td>val_loss_epoch</td><td>▃▄▅▄▃▂▅▇█▇▂▁▂▂▂▂▁▁▁▂▅▃▄▄▆▇▆▆▅▃▁▁▁▁▃▇█▄▁▁</td></tr><tr><td>val_loss_step</td><td>▃▄▅▄▃▂▅▇█▇▂▁▂▂▂▂▁▁▁▂▅▃▄▄▆▇▆▆▅▃▁▁▁▁▃▇█▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.54545</td></tr><tr><td>test_auc</td><td>0.4</td></tr><tr><td>test_f1</td><td>0.70588</td></tr><tr><td>train_acc</td><td>0.88889</td></tr><tr><td>train_auc</td><td>0.925</td></tr><tr><td>train_f1</td><td>0.88889</td></tr><tr><td>train_loss_epoch</td><td>0.45308</td></tr><tr><td>train_loss_step</td><td>0.45308</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>1.0</td></tr><tr><td>val_auc</td><td>1.0</td></tr><tr><td>val_f1</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.31595</td></tr><tr><td>val_loss_step</td><td>0.31595</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bwo4ngns' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/bwo4ngns</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200747-bwo4ngns\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200815-rflaqwur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rflaqwur' target=\"_blank\">MLP_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rflaqwur' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rflaqwur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\attention\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | GlobalAttention  | 33     | train\n",
      "----------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.4545), 'test_auc': tensor(0.6000), 'test_f1': tensor(0.)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▃▂▂▂▄▅▆▄▄▄▅▄▃▂▄▇▄▄▂▇▃▇▄▅▄▇▇▆█▆▅▅▅▄▅▇▇▄</td></tr><tr><td>train_auc</td><td>▃▅▃▁▁▃▃▅█▆▆▄▄▇▆▃▇▇▆▆▄█▆▇▆▇▆█▇▆██▆▇█▇▆▇▇▆</td></tr><tr><td>train_f1</td><td>▅▅▃▁▁▁▅▆▆▄▃▅▆▆▅▄▄▇▆▅▂▇▄▇▅▆▅▇▇▆█▆▆▆▆▅▆▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▆▇▆▅▄▃▄▅▅▅▅▅▆▃▃▄▅▆▂▄▂▃▃▄▂▃▃▁▃▃▄▃▄▄▂▂▄</td></tr><tr><td>train_loss_step</td><td>▆█▅▇▅▁▂▁▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▁▁████▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>val_f1</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▃▅▆▆▆▅▆▆▇▇▇▆▅▆█▇▇▇▇████▇▇▇▆▇▇▇▇▅▅▇█████</td></tr><tr><td>val_loss_step</td><td>▁▃▅▆▆▆▅▆▆▇▇▇▆▅▆█▇▇▇▇████▇▇▇▆▇▇▇▇▅▅▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.45455</td></tr><tr><td>test_auc</td><td>0.6</td></tr><tr><td>test_f1</td><td>0.0</td></tr><tr><td>train_acc</td><td>0.72222</td></tr><tr><td>train_auc</td><td>0.85</td></tr><tr><td>train_f1</td><td>0.7619</td></tr><tr><td>train_loss_epoch</td><td>0.5976</td></tr><tr><td>train_loss_step</td><td>0.5976</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.2</td></tr><tr><td>val_auc</td><td>0.75</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>1.10494</td></tr><tr><td>val_loss_step</td><td>1.10494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rflaqwur' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/rflaqwur</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200815-rflaqwur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\notebooks\\wandb\\run-20240815_200841-uboqxvjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/uboqxvjg' target=\"_blank\">MLP_3_32_4</a></strong> to <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/uboqxvjg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/uboqxvjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "Missing logger folder: Y:\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\attention2\\GraphLevelMLP\\lightning_logs\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory \\\\bme-data2.ad.gatech.edu\\labs5\\coskun-lab\\Efe and Nishkala\\SnowflakePipeline\\11_snowflakes\\data\\saved_models\\Graph_GNNs_SF_15082024_tls_snowflake_pos_4\\MLP_3_32_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0  | model       | MLPModel         | 4.1 K  | train\n",
      "1  | fnn_layer   | Linear           | 1.1 K  | train\n",
      "2  | selu        | SELU             | 0      | train\n",
      "3  | head        | Linear           | 66     | train\n",
      "4  | loss_module | CrossEntropyLoss | 0      | train\n",
      "5  | train_acc   | BinaryAccuracy   | 0      | train\n",
      "6  | train_auroc | BinaryAUROC      | 0      | train\n",
      "7  | train_f1    | BinaryF1Score    | 0      | train\n",
      "8  | valid_acc   | BinaryAccuracy   | 0      | train\n",
      "9  | valid_auroc | BinaryAUROC      | 0      | train\n",
      "10 | valid_f1    | BinaryF1Score    | 0      | train\n",
      "11 | test_acc    | BinaryAccuracy   | 0      | train\n",
      "12 | test_auroc  | BinaryAUROC      | 0      | train\n",
      "13 | test_f1     | BinaryF1Score    | 0      | train\n",
      "14 | pool        | Attention_module | 1.1 K  | train\n",
      "----------------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\torch_gpu2\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': tensor(0.6364), 'test_auc': tensor(0.7333), 'test_f1': tensor(0.7143)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▃▅▃▅▆▆▅▆▆▇█▆▇██▇████████████████████▇█</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇▆██▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▄▆▅▆▇▇▆▇▆██▇██████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▅▅▃▃▄▄▄▂▁▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▂▁▁▂▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▁▁▁███████▅█▅▅▅▅▅▅▅▅▅▅▅██▅▅▅██▅▅▅▅█▅▅</td></tr><tr><td>val_auc</td><td>▁▅▅▅▅▅▅▅█████▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁███████▆█▆▆▆▆▆▆▆▆▆▆▆██▆▆▆██▆▆▆▆█▆▆</td></tr><tr><td>val_loss_epoch</td><td>▄▅▇▆██▃▄▂▁▂▂▁▂▂▄▄▅▅▅▅▅▅▅▅▄▃▃▅▅▅▃▃▄▅▅▄▃▅▅</td></tr><tr><td>val_loss_step</td><td>▄▅▇▆██▃▄▂▁▂▂▁▂▂▄▄▅▅▅▅▅▅▅▅▄▃▃▅▅▅▃▃▄▅▅▄▃▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>test_acc</td><td>0.63636</td></tr><tr><td>test_auc</td><td>0.73333</td></tr><tr><td>test_f1</td><td>0.71429</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_auc</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.34078</td></tr><tr><td>train_loss_step</td><td>0.34078</td></tr><tr><td>trainer/global_step</td><td>100</td></tr><tr><td>val_acc</td><td>0.6</td></tr><tr><td>val_auc</td><td>0.75</td></tr><tr><td>val_f1</td><td>0.75</td></tr><tr><td>val_loss_epoch</td><td>0.69474</td></tr><tr><td>val_loss_step</td><td>0.69474</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_4</strong> at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/uboqxvjg' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4/runs/uboqxvjg</a><br/> View project at: <a href='https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4' target=\"_blank\">https://wandb.ai/efesthefirst234/SF_15082024_tls_snowflake_pos_4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_200841-uboqxvjg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print K fold model number of samples and number of positive cases\n",
    "\n",
    "k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "kfold = StratifiedShuffleSplit(n_splits=k_folds, test_size=0.3, random_state=0)\n",
    "val_size = 0.2\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, ys)):\n",
    "    # train_subset = dataset.index_select(train_ids.tolist())\n",
    "    test_subset = dataset.index_select(test_ids.tolist())\n",
    "    \n",
    "    train_ids, valid_ids = train_test_split(train_ids, test_size = val_size, random_state=0)\n",
    "    train_subset = dataset.index_select(train_ids.tolist())\n",
    "    val_subset = dataset.index_select(valid_ids.tolist())\n",
    "    \n",
    "    for pool in pools:\n",
    "        for model in models:\n",
    "            if model == 'GAT':\n",
    "                batch_size=64\n",
    "            else:\n",
    "                batch_size=128\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "            CHECKPOINT_PATH = checkpoint_folder / f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}' / pool\n",
    "            CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Skip already trained kfold and pool\n",
    "            checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "            if checkpoint.exists():\n",
    "                print(checkpoint)\n",
    "                continue\n",
    "\n",
    "            # Run training\n",
    "            run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_{fold}', \n",
    "                            group=f'{model}_{pool}')\n",
    "            graph.train_graph_classifier_kfold(model, \n",
    "                                                 train_subset, \n",
    "                                                 val_subset, \n",
    "                                                 test_subset,\n",
    "                                                 dataset, \n",
    "                                                 CHECKPOINT_PATH, \n",
    "                                                 AVAIL_GPUS, \n",
    "                                                 hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                 num_layers=NUM_LAYERS, \n",
    "                                                 epochs=epochs,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 graph_pooling=pool,\n",
    "                                                 morph=False,\n",
    "                                                 pos=True)\n",
    "            run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "torch_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
